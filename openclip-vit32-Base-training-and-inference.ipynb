{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9778892,"sourceType":"datasetVersion","datasetId":5990648},{"sourceId":9828026,"sourceType":"datasetVersion","datasetId":6027298}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CLIPProcessor, CLIPModel\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport os\n\n\nimage_dir = '/kaggle/input/mesho-chll/MESHO/train_images/'\ncsv_file = '/kaggle/input/mesho-chll/MESHO/train_MESH.csv'\n\n\nbatch_size = 64\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndf = pd.read_csv(csv_file)\nattr_columns = [f'attr_{i}' for i in range(1, 11)]\ndf[attr_columns] = df[attr_columns].fillna('no')\n\ncategory_encoder = LabelEncoder()\ndf['Category'] = category_encoder.fit_transform(df['Category'])\n\nattr_encoders = {}\nfor col in attr_columns:\n    encoder = LabelEncoder()\n    df[col] = encoder.fit_transform(df[col])\n    attr_encoders[col] = encoder\n\ndf['file_path'] = df['id'].apply(lambda x: f'{image_dir}{str(x).zfill(6)}.jpg')\n\n\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_model.to(device)\nclip_model.eval()\n\nclass CustomDataset(Dataset):\n    def __init__(self, df, processor):\n        self.df = df\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = self.df.iloc[idx]['file_path']\n        image = Image.open(img_path).convert('RGB')\n        inputs = self.processor(images=image, return_tensors=\"pt\")\n        features = self.df.iloc[idx][['Category']].values.astype(np.float32)\n        labels = self.df.iloc[idx][attr_columns].values.astype(np.int64)\n        \n        # Move inputs to the correct device\n        inputs = {k: v.squeeze(0) for k, v in inputs.items()}    # Move labels to device\n        \n        return (inputs, features), labels\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_df, val_df = train_test_split(df, test_size=0.4, random_state=42)\n\ndef custom_dataloader(df, processor, batch_size, shuffle=True):\n    dataset = CustomDataset(df, processor)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n\n\ntrain_loader = custom_dataloader(train_df, clip_processor, batch_size)\nval_loader = custom_dataloader(val_df, clip_processor, batch_size, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T21:44:58.370917Z","iopub.execute_input":"2024-11-06T21:44:58.371394Z","iopub.status.idle":"2024-11-06T21:44:59.650339Z","shell.execute_reply.started":"2024-11-06T21:44:58.371349Z","shell.execute_reply":"2024-11-06T21:44:59.649450Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"class MultiOutputModel(nn.Module):\n    def __init__(self, clip_model, num_features, num_outputs_per_attr):\n        super(MultiOutputModel, self).__init__()\n        \n        self.clip_model = clip_model\n        self.clip_model.eval()  \n\n        \n        clip_output_dim = 512  \n        self.fc_features = nn.Linear(clip_output_dim + num_features, 512)\n        self.fc1 = nn.Linear(512, 256)\n\n      \n        self.attr_outputs = nn.ModuleList([nn.Linear(256, num_outputs) for num_outputs in num_outputs_per_attr])\n\n    def forward(self, inputs, features):\n        image_inputs = inputs['pixel_values'].to(device)  \n\n\n        with torch.no_grad():\n            image_embeddings = self.clip_model.get_image_features(image_inputs)\n\n    \n        min_batch_size = min(image_embeddings.size(0), features.size(0))\n        image_embeddings = image_embeddings[:min_batch_size]\n        features = features[:min_batch_size]\n\n     \n        x = torch.cat([image_embeddings, features], dim=1)\n\n        \n        x = torch.relu(self.fc_features(x))\n        x = torch.relu(self.fc1(x))\n        outputs = [attr_output(x) for attr_output in self.attr_outputs]\n\n        return outputs\n\nnum_features = 1\nnum_classes_list = [len(attr_encoders[f'attr_{i}'].classes_) for i in range(1, 11)]\nmodel = MultiOutputModel(clip_model, num_features=num_features, num_outputs_per_attr=num_classes_list)\nmodel = model.to(device)\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T21:44:19.459054Z","iopub.execute_input":"2024-11-06T21:44:19.459507Z","iopub.status.idle":"2024-11-06T21:44:19.489303Z","shell.execute_reply.started":"2024-11-06T21:44:19.459459Z","shell.execute_reply":"2024-11-06T21:44:19.488164Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def compute_accuracy(outputs, labels):\n    \"\"\"\n    Compute accuracy for multi-output model.\n    Args:\n    outputs (list of tensors): Model outputs for each attribute.\n    labels (tensor): Ground truth labels.\n    \"\"\"\n    correct = 0\n    total = 0\n    for output, label in zip(outputs, labels.T):  \n        preds = output.argmax(dim=1)\n        correct += (preds == label).sum().item()\n        total += label.size(0)\n    return correct / total","metadata":{"execution":{"iopub.status.busy":"2024-11-06T21:44:22.284520Z","iopub.execute_input":"2024-11-06T21:44:22.285177Z","iopub.status.idle":"2024-11-06T21:44:22.290803Z","shell.execute_reply.started":"2024-11-06T21:44:22.285132Z","shell.execute_reply":"2024-11-06T21:44:22.289769Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef load_model_checkpoint(model, checkpoint_path):\n    \"\"\"Load the model from the specified checkpoint path.\"\"\"\n    if os.path.exists(checkpoint_path):\n        model.load_state_dict(torch.load(checkpoint_path))\n        print(f\"Model loaded from {checkpoint_path}\")\n    else:\n        print(f\"Checkpoint not found at {checkpoint_path}\")\n\ndef compute_f1(outputs, labels):\n    \"\"\"\n    Compute F1 score for multi-output model.\n    Args:\n    outputs (list of tensors): Model outputs for each attribute.\n    labels (tensor): Ground truth labels.\n    \"\"\"\n    all_preds = []\n    all_labels = []\n    for output, label in zip(outputs, labels.T): \n        preds = output.argmax(dim=1).cpu()\n        all_preds.extend(preds.numpy())\n        all_labels.extend(label.cpu().numpy())\n    return f1_score(all_labels, all_preds, average='micro')\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, epochs=15, save_dir='model_checkpoints', checkpoint_path=None):\n    os.makedirs(save_dir, exist_ok=True)\n    model.to(device)\n    \n   \n    if checkpoint_path:\n        load_model_checkpoint(model, checkpoint_path)\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        train_f1_scores = []\n\n        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{epochs}', unit='batch') as pbar:\n            for batch_idx, ((inputs, features), labels) in enumerate(train_loader):\n                inputs = {k: v.to(device) for k, v in inputs.items()}\n                features = features.to(device)\n                labels = labels.to(device)\n                \n                optimizer.zero_grad()\n                \n       \n                outputs = model(inputs, features)\n                labels = labels.long()\n                total_loss = 0.0\n\n                for idx, (output, label) in enumerate(zip(outputs, labels.T)):\n                    loss = criterion(output, label)\n                    total_loss += loss\n\n       \n                total_loss.backward()\n                optimizer.step()\n\n                running_loss += total_loss.item()\n                batch_accuracy = compute_accuracy(outputs, labels)\n                train_correct += batch_accuracy * len(labels)\n                train_total += len(labels)\n                train_f1_scores.append(compute_f1(outputs, labels))\n                \n                pbar.set_postfix({'Batch Loss': total_loss.item(), 'Train Acc': batch_accuracy})\n                pbar.update(1)\n\n        avg_train_loss = running_loss / len(train_loader)\n        avg_train_accuracy = train_correct / train_total\n        avg_train_f1 = sum(train_f1_scores) / len(train_f1_scores)\n\n\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        val_f1_scores = []\n        with torch.no_grad():\n            for (inputs, features), labels in val_loader:\n                inputs = {k: v.to(device) for k, v in inputs.items()}\n                features = features.to(device)\n                labels = labels.to(device)\n\n                outputs = model(inputs, features)\n                labels = labels.long()\n                \n                total_loss = 0.0\n                for idx, (output, label) in enumerate(zip(outputs, labels.T)):\n                    loss = criterion(output, label)\n                    total_loss += loss\n\n                val_loss += total_loss.item()\n                batch_accuracy = compute_accuracy(outputs, labels)\n                val_correct += batch_accuracy * len(labels)\n                val_total += len(labels)\n                val_f1_scores.append(compute_f1(outputs, labels))\n\n        avg_val_loss = val_loss / len(val_loader)\n        avg_val_accuracy = val_correct / val_total\n        avg_val_f1 = sum(val_f1_scores) / len(val_f1_scores)\n\n        print(f'Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f} | Train Acc: {avg_train_accuracy:.4f} | Train F1: {avg_train_f1:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {avg_val_accuracy:.4f} | Val F1: {avg_val_f1:.4f}')\n\n      \n        checkpoint_path = os.path.join(save_dir, f'model_epoch_{epoch + 1}.pth')\n        torch.save(model.state_dict(), checkpoint_path)\n        print(f'Model saved at {checkpoint_path}')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T21:44:24.204556Z","iopub.execute_input":"2024-11-06T21:44:24.205404Z","iopub.status.idle":"2024-11-06T21:44:24.227267Z","shell.execute_reply.started":"2024-11-06T21:44:24.205361Z","shell.execute_reply":"2024-11-06T21:44:24.226202Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,  \n    criterion=criterion,\n    optimizer=optimizer,\n    epochs=15,\n\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T21:45:03.441359Z","iopub.execute_input":"2024-11-06T21:45:03.442362Z","iopub.status.idle":"2024-11-06T23:03:40.592647Z","shell.execute_reply.started":"2024-11-06T21:45:03.442307Z","shell.execute_reply":"2024-11-06T23:03:40.591658Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Epoch 1/15: 100%|██████████| 172/172 [05:02<00:00,  1.76s/batch, Batch Loss=7.83, Train Acc=0.706]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 8.9891 | Train Acc: 0.6466 | Train F1: 0.6466 | Val Loss: 7.9573 | Val Acc: 0.6813 | Val F1: 0.6814\nModel saved at model_checkpoints/model_epoch_1.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/15: 100%|██████████| 172/172 [02:56<00:00,  1.03s/batch, Batch Loss=6.98, Train Acc=0.724]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 7.8158 | Train Acc: 0.6818 | Train F1: 0.6818 | Val Loss: 7.6417 | Val Acc: 0.6870 | Val F1: 0.6871\nModel saved at model_checkpoints/model_epoch_2.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/15: 100%|██████████| 172/172 [02:56<00:00,  1.03s/batch, Batch Loss=7.51, Train Acc=0.679]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 7.5655 | Train Acc: 0.6876 | Train F1: 0.6876 | Val Loss: 7.5686 | Val Acc: 0.6861 | Val F1: 0.6862\nModel saved at model_checkpoints/model_epoch_3.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/15: 100%|██████████| 172/172 [02:58<00:00,  1.04s/batch, Batch Loss=7.92, Train Acc=0.694]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 7.4224 | Train Acc: 0.6898 | Train F1: 0.6898 | Val Loss: 7.4561 | Val Acc: 0.6873 | Val F1: 0.6874\nModel saved at model_checkpoints/model_epoch_4.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/15: 100%|██████████| 172/172 [02:59<00:00,  1.04s/batch, Batch Loss=7, Train Acc=0.687]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 7.3276 | Train Acc: 0.6933 | Train F1: 0.6933 | Val Loss: 7.4346 | Val Acc: 0.6905 | Val F1: 0.6905\nModel saved at model_checkpoints/model_epoch_5.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/15: 100%|██████████| 172/172 [02:58<00:00,  1.04s/batch, Batch Loss=6.9, Train Acc=0.713] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Train Loss: 7.2453 | Train Acc: 0.6976 | Train F1: 0.6976 | Val Loss: 7.3682 | Val Acc: 0.6941 | Val F1: 0.6941\nModel saved at model_checkpoints/model_epoch_6.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/15: 100%|██████████| 172/172 [02:58<00:00,  1.04s/batch, Batch Loss=7.58, Train Acc=0.684]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Train Loss: 7.1853 | Train Acc: 0.6990 | Train F1: 0.6990 | Val Loss: 7.3680 | Val Acc: 0.6947 | Val F1: 0.6947\nModel saved at model_checkpoints/model_epoch_7.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/15: 100%|██████████| 172/172 [03:36<00:00,  1.26s/batch, Batch Loss=7.21, Train Acc=0.714]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Train Loss: 7.1285 | Train Acc: 0.7017 | Train F1: 0.7017 | Val Loss: 7.3105 | Val Acc: 0.6955 | Val F1: 0.6956\nModel saved at model_checkpoints/model_epoch_8.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/15: 100%|██████████| 172/172 [02:58<00:00,  1.04s/batch, Batch Loss=7.42, Train Acc=0.702]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Train Loss: 7.0780 | Train Acc: 0.7023 | Train F1: 0.7023 | Val Loss: 7.3962 | Val Acc: 0.6926 | Val F1: 0.6926\nModel saved at model_checkpoints/model_epoch_9.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/15: 100%|██████████| 172/172 [03:04<00:00,  1.07s/batch, Batch Loss=7.25, Train Acc=0.698]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 7.0489 | Train Acc: 0.7039 | Train F1: 0.7039 | Val Loss: 7.3529 | Val Acc: 0.6925 | Val F1: 0.6926\nModel saved at model_checkpoints/model_epoch_10.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/15: 100%|██████████| 172/172 [03:02<00:00,  1.06s/batch, Batch Loss=6.19, Train Acc=0.794]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 7.0034 | Train Acc: 0.7054 | Train F1: 0.7054 | Val Loss: 7.3462 | Val Acc: 0.6958 | Val F1: 0.6959\nModel saved at model_checkpoints/model_epoch_11.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/15: 100%|██████████| 172/172 [03:03<00:00,  1.07s/batch, Batch Loss=7.27, Train Acc=0.703]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 6.9696 | Train Acc: 0.7062 | Train F1: 0.7062 | Val Loss: 7.3414 | Val Acc: 0.6962 | Val F1: 0.6962\nModel saved at model_checkpoints/model_epoch_12.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/15: 100%|██████████| 172/172 [02:57<00:00,  1.03s/batch, Batch Loss=6.85, Train Acc=0.695]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Train Loss: 6.9248 | Train Acc: 0.7084 | Train F1: 0.7084 | Val Loss: 7.3397 | Val Acc: 0.6935 | Val F1: 0.6937\nModel saved at model_checkpoints/model_epoch_13.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/15: 100%|██████████| 172/172 [02:57<00:00,  1.03s/batch, Batch Loss=6.88, Train Acc=0.697]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Train Loss: 6.8938 | Train Acc: 0.7089 | Train F1: 0.7089 | Val Loss: 7.3489 | Val Acc: 0.6969 | Val F1: 0.6970\nModel saved at model_checkpoints/model_epoch_14.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/15: 100%|██████████| 172/172 [02:57<00:00,  1.03s/batch, Batch Loss=7.04, Train Acc=0.683]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Train Loss: 6.8713 | Train Acc: 0.7106 | Train F1: 0.7106 | Val Loss: 7.3434 | Val Acc: 0.6976 | Val F1: 0.6977\nModel saved at model_checkpoints/model_epoch_15.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the test data\ntest_df = pd.read_csv('/kaggle/input/mesho-chll/MESHO/test.csv')\n\n# Filter for rows where Category is 'Women Tops & Tunics'\nsaree_df = test_df[test_df['Category'] == 'Sarees']\n\n# Set the Category column to 0 for the filtered dataframe\n# women_tops_tunics_df['Category'] = 0\n\n# Save the resulting dataframe to CSV\nsaree_df.to_csv('saree_cat2.csv', index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:05:13.869357Z","iopub.execute_input":"2024-11-06T23:05:13.870038Z","iopub.status.idle":"2024-11-06T23:05:13.907335Z","shell.execute_reply.started":"2024-11-06T23:05:13.869996Z","shell.execute_reply":"2024-11-06T23:05:13.906354Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nfrom transformers import CLIPProcessor, CLIPModel\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm  # Import tqdm for progress bar\n\ndef load_trained_model(model_path, clip_model, num_features, num_classes_list):\n    # Load the state dictionary from the model path\n    state_dict = torch.load(model_path)\n\n#     if list(state_dict.keys())[0].startswith(\"module.\"):\n#         # Remove 'module.' prefix if present\n#         state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n    \n    # Create the model instance before loading the state dictionary\n    model = MultiOutputModel(clip_model, num_features=num_features, num_outputs_per_attr=num_classes_list)\n    \n    # Load the state dictionary with strict=False to avoid key mismatches\n    model.load_state_dict(state_dict)\n    \n    model.to(device)\n    model.eval()  # Set to evaluation mode\n    return model\n\n\n# Paths and parameters\nmodel_path = '/kaggle/working/model_checkpoints/model_epoch_15.pth'  # Path to your trained model\nimage_dir = '/kaggle/input/mesho-chll/MESHO/test_images/'  # Path to test images directory\ncsv_file = '/kaggle/working/saree_cat2.csv'  # Path to test CSV file\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load the CLIP model and processor\n# Load the CLIP model and processor\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n# Assuming the same encoding for the attributes used during training\nnum_features = 1  # 'Category'\nnum_classes_list = [len(attr_encoders[f'attr_{i}'].classes_) for i in range(1, 11)]\n\n# Load the model with the trained weights\nmodel = load_trained_model(model_path, clip_model, num_features, num_classes_list)\n\n# Load the test CSV that contains the 'id' and 'Category'\ntest_df = pd.read_csv(csv_file)\n\n# Preprocess a single image for inference\ndef preprocess_image(image_path, processor):\n    image = Image.open(image_path).convert('RGB')\n    inputs = processor(images=image, return_tensors=\"pt\")\n    return inputs\n\n# Inference function\ndef predict(model, image_path, processor, features):\n    # Preprocess the image\n    inputs = preprocess_image(image_path, processor)\n    inputs = inputs.to(device)\n\n    # Ensure features are properly shaped for model input\n    features = torch.tensor(features).unsqueeze(0).to(device)  # Shape as (1, num_features)\n\n    # Perform forward pass (inference)\n    with torch.no_grad():\n        attr_outputs = model(inputs, features)\n\n    # Convert outputs to predicted labels\n    predicted_labels = [torch.argmax(output, dim=1).item() for output in attr_outputs]\n   \n    return predicted_labels\n\n# Example: Perform inference on all test images\npredictions_list = []\n\n# Wrap the loop with tqdm for progress bar\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing Images\"):\n    image_id = str(row['id']).zfill(6) + '.jpg'  # Convert ID to format 000000.jpg\n    image_path = os.path.join(image_dir, image_id)\n    # Encode the Category (make sure it was encoded similarly as in training)\n    category_encoded_value = 0\n    \n    # Perform prediction\n    predicted_attrs = predict(model, image_path, clip_processor, [category_encoded_value])\n\n    # Decode the predicted attributes back to their original labels\n    decoded_predictions = {f'attr_{i}': attr_encoders[f'attr_{i}'].inverse_transform([pred])[0] \n                           for i, pred in enumerate(predicted_attrs, 1)}\n    \n    # Store the results for this image, without Category for now\n    predictions_list.append({'id': row['id'], **decoded_predictions})\n\n# Convert predictions to a DataFrame for better output readability\npredictions_df = pd.DataFrame(predictions_list)\n\n# Merge predictions with the original test DataFrame based on 'id'\nmerged_df = pd.merge(test_df[['id', 'Category']], predictions_df, on='id')\n\n# Count attributes that are not predicted as 'no'\n# Assuming attribute columns are named attr_1, attr_2, ..., attr_10\nattribute_columns = [f'attr_{i}' for i in range(1, 11)]\n\n# Create the 'len' column based on the count of attributes that are not 'no'\nmerged_df['len'] = merged_df[attribute_columns].apply(lambda x: sum(attr != 'no' for attr in x), axis=1)\n\n# Reorder columns to have 'len' after 'Category' and before 'attr_1'\ncols = ['id', 'Category', 'len'] + attribute_columns\nmerged_df = merged_df[cols]\n\n# Display merged predictions for all test images\nprint(merged_df)\n\n# Save predictions to a CSV file\nmerged_df.to_csv('cat_2_pred.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:05:47.994103Z","iopub.execute_input":"2024-11-06T23:05:47.994516Z","iopub.status.idle":"2024-11-06T23:10:00.819784Z","shell.execute_reply.started":"2024-11-06T23:05:47.994467Z","shell.execute_reply":"2024-11-06T23:10:00.818878Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/tmp/ipykernel_30/1118361313.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(model_path)\nProcessing Images: 100%|██████████| 7102/7102 [04:10<00:00, 28.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"         id Category  len         attr_1        attr_2        attr_3  \\\n0      3787   Sarees    8        default       default  small border   \n1      3788   Sarees    6             no  woven design            no   \n2      3789   Sarees    9  same as saree          zari    big border   \n3      3790   Sarees    7             no  woven design    big border   \n4      3791   Sarees    4             no  woven design            no   \n...     ...      ...  ...            ...           ...           ...   \n7097  11150   Sarees    4             no  woven design            no   \n7098  11151   Sarees    1             no            no    big border   \n7099  11152   Sarees    6             no  woven design            no   \n7100  11153   Sarees    6             no  woven design    big border   \n7101  11154   Sarees    6             no          zari  small border   \n\n          attr_4       attr_5    attr_6      attr_7      attr_8        attr_9  \\\n0        default        party   default          no     default       default   \n1        default  traditional        no  zari woven  zari woven  ethnic motif   \n2        default        party  jacquard  zari woven       solid         solid   \n3        default  traditional        no  zari woven  zari woven       default   \n4     multicolor  traditional        no          no          no       default   \n...          ...          ...       ...         ...         ...           ...   \n7097  multicolor  traditional        no          no          no       default   \n7098          no           no        no          no          no            no   \n7099     default        daily        no  zari woven  zari woven       default   \n7100     default  traditional        no          no  zari woven       default   \n7101       cream        party        no          no  zari woven       peacock   \n\n     attr_10  \n0         no  \n1         no  \n2         no  \n3         no  \n4         no  \n...      ...  \n7097      no  \n7098      no  \n7099      no  \n7100      no  \n7101      no  \n\n[7102 rows x 13 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nfrom transformers import CLIPProcessor, CLIPModel\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm  # Import tqdm for progress bar\n\ndef load_trained_model(model_path, clip_model, num_features, num_classes_list):\n    # Load the state dictionary from the model path\n    state_dict = torch.load(model_path)\n\n#     if list(state_dict.keys())[0].startswith(\"module.\"):\n#         # Remove 'module.' prefix if present\n#         state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n    \n    # Create the model instance before loading the state dictionary\n    model = MultiOutputModel(clip_model, num_features=num_features, num_outputs_per_attr=num_classes_list)\n    \n    # Load the state dictionary with strict=False to avoid key mismatches\n    model.load_state_dict(state_dict)\n    \n    model.to(device)\n    model.eval()  # Set to evaluation mode\n    return model\n\n\n# Paths and parameters\nmodel_path = '/kaggle/working/model_checkpoints/model_epoch_clip_base.pth'  # Path to your trained model\nimage_dir = '/kaggle/input/meesho-chll/MESHO/test_images/'  # Path to test images directory\ncsv_file = '/kaggle/input/meesho-chll/MESHO/test.csv'  # Path to test CSV file\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n# Load the CLIP model and processor\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n# Assuming the same encoding for the attributes used during training\nnum_features = 1  # 'Category'\nnum_classes_list = [len(attr_encoders[f'attr_{i}'].classes_) for i in range(1, 11)]\n\n# Load the model with the trained weights\nmodel = load_trained_model(model_path, clip_model, num_features, num_classes_list)\n\n\ntest_df = pd.read_csv(csv_file)\n\n\ndef preprocess_image(image_path, processor):\n    image = Image.open(image_path).convert('RGB')\n    inputs = processor(images=image, return_tensors=\"pt\")\n    return inputs\n\n\ndef predict(model, image_path, processor, features):\n\n    inputs = preprocess_image(image_path, processor)\n    inputs = inputs.to(device)\n\n\n    features = torch.tensor(features).unsqueeze(0).to(device)  # Shape as (1, num_features)\n\n    \n    with torch.no_grad():\n        attr_outputs = model(inputs, features)\n\n    \n    predicted_labels = [torch.argmax(output, dim=1).item() for output in attr_outputs]\n   \n    return predicted_labels\n\npredictions_list = []\n\n\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing Images\"):\n    image_id = str(row['id']).zfill(6) + '.jpg' \n    image_path = os.path.join(image_dir, image_id)\n\n   \n    category_encoded_value = category_encoder.transform([row['Category']])[0]\n\n    predicted_attrs = predict(model, image_path, clip_processor, [category_encoded_value])\n\n\n    decoded_predictions = {f'attr_{i}': attr_encoders[f'attr_{i}'].inverse_transform([pred])[0] \n                           for i, pred in enumerate(predicted_attrs, 1)}\n\n    predictions_list.append({'id': row['id'], **decoded_predictions})\n\n\npredictions_df = pd.DataFrame(predictions_list)\n\n\nmerged_df = pd.merge(test_df[['id', 'Category']], predictions_df, on='id')\n\n\nattribute_columns = [f'attr_{i}' for i in range(1, 11)]\n\n\nmerged_df['len'] = merged_df[attribute_columns].apply(lambda x: sum(attr != 'no' for attr in x), axis=1)\n\n\ncols = ['id', 'Category', 'len'] + attribute_columns\nmerged_df = merged_df[cols]\n\n\nprint(merged_df)\n\n\nmerged_df.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T16:46:38.800880Z","iopub.execute_input":"2024-11-01T16:46:38.801258Z","iopub.status.idle":"2024-11-01T17:00:49.120461Z","shell.execute_reply.started":"2024-11-01T16:46:38.801223Z","shell.execute_reply":"2024-11-01T17:00:49.119484Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2158838204.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(model_path)\nProcessing Images: 100%|██████████| 30205/30205 [14:07<00:00, 35.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"          id             Category  len      attr_1  attr_2   attr_3  \\\n0          0          Men Tshirts    2  multicolor   round       no   \n1          1          Men Tshirts    3          no   round  printed   \n2          2          Men Tshirts    1          no      no       no   \n3          3          Men Tshirts    5  multicolor    polo    solid   \n4          4          Men Tshirts    1          no   round       no   \n...      ...                  ...  ...         ...     ...      ...   \n30200  30484  Women Tops & Tunics    9       green    boxy     crop   \n30201  30485  Women Tops & Tunics    0          no      no       no   \n30202  30486  Women Tops & Tunics    3          no      no  regular   \n30203  30487  Women Tops & Tunics    3          no      no       no   \n30204  30488  Women Tops & Tunics    9        pink  fitted     crop   \n\n           attr_4         attr_5   attr_6      attr_7         attr_8  \\\n0              no             no       no          no             no   \n1      typography             no       no          no             no   \n2         default             no       no          no             no   \n3           solid  short sleeves       no          no             no   \n4              no             no       no          no             no   \n...           ...            ...      ...         ...            ...   \n30200  round neck         casual  printed  typography  short sleeves   \n30201          no             no       no          no             no   \n30202          no             no    solid          no  short sleeves   \n30203          no             no  default     default   long sleeves   \n30204     default         casual    solid       solid     sleeveless   \n\n                attr_9 attr_10  \n0                   no      no  \n1                   no      no  \n2                   no      no  \n3                   no      no  \n4                   no      no  \n...                ...     ...  \n30200  regular sleeves      no  \n30201               no      no  \n30202               no      no  \n30203               no      no  \n30204       sleeveless      no  \n\n[30205 rows x 13 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}