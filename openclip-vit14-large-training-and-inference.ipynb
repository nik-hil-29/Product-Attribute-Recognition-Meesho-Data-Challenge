{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9778892,"sourceType":"datasetVersion","datasetId":5990648},{"sourceId":9802013,"sourceType":"datasetVersion","datasetId":6007591},{"sourceId":9805899,"sourceType":"datasetVersion","datasetId":6010563}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CLIPProcessor, CLIPModel\nfrom sklearn.preprocessing import LabelEncoder\nimport os\n\n# Paths\nimage_dir = '/kaggle/input/mesho-chll/MESHO/train_images/'  # Path to train images\ncsv_file = '//kaggle/input/mesho-chll/MESHO/train_MESH.csv'       # Path to CSV file\n\n# Parameters\nbatch_size = 64\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 1. Load the CSV that contains image IDs, Category, and target labels (attr_1 to attr_10)\ndf = pd.read_csv(csv_file)\n\n# Fill NaN values in attr_1 to attr_10 with 'no'\nattr_columns = [f'attr_{i}' for i in range(1, 11)]\ndf[attr_columns] = df[attr_columns].fillna('no')\n\n# Label encode the 'Category' column\ncategory_encoder = LabelEncoder()\ndf['Category'] = category_encoder.fit_transform(df['Category'])\n\n# Encode each attribute (attr_1 to attr_10) using LabelEncoder\nattr_encoders = {}\nfor col in attr_columns:\n    encoder = LabelEncoder()\n    df[col] = encoder.fit_transform(df[col])\n    attr_encoders[col] = encoder  # Store the encoder for later use\n\n# Create a new column for the full file path of each image\ndf['file_path'] = df['id'].apply(lambda x: f'{image_dir}{str(x).zfill(6)}.jpg')\n\n# 2. Load the CLIP model and processor\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n\nclip_model.to(device)\nclip_model.eval()  # Set to evaluation mode since we only use the encoders\n\n# Custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, df, processor):\n        self.df = df\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = self.df.iloc[idx]['file_path']\n        image = Image.open(img_path).convert('RGB')\n\n        # Use CLIP processor to process the image (resizing, normalization)\n        inputs = self.processor(images=image, return_tensors=\"pt\")\n\n        # Get additional features (Category, len)\n        features = self.df.iloc[idx][['Category']].values.astype(np.float32)\n\n        # Get labels for attributes\n        labels = self.df.iloc[idx][attr_columns].values.astype(np.int64)\n\n        return (inputs, features), torch.tensor(labels)  # Return inputs for multi-label classification\n\n# 3. DataLoader for training and validation\ndef custom_dataloader(df, processor, batch_size):\n    dataset = CustomDataset(df, processor)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\ntrain_loader = custom_dataloader(df, clip_processor, batch_size)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-04T09:59:34.718953Z","iopub.execute_input":"2024-11-04T09:59:34.719884Z","iopub.status.idle":"2024-11-04T09:59:36.881104Z","shell.execute_reply.started":"2024-11-04T09:59:34.719841Z","shell.execute_reply":"2024-11-04T09:59:36.880234Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\n\n# Suppress all warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T09:59:47.984426Z","iopub.execute_input":"2024-11-04T09:59:47.984817Z","iopub.status.idle":"2024-11-04T09:59:47.989181Z","shell.execute_reply.started":"2024-11-04T09:59:47.984782Z","shell.execute_reply":"2024-11-04T09:59:47.988221Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# 4. Define the multi-output model using CLIP encodings\nimport torch.optim as optim\nfrom torch.cuda.amp import GradScaler, autocast\nclass MultiOutputModel(nn.Module):\n    def __init__(self, clip_model, num_features, num_outputs_per_attr):\n        super(MultiOutputModel, self).__init__()\n        \n        self.clip_model = clip_model\n        self.clip_model.eval()  # Set CLIP to eval mode\n\n        # Custom fully connected layers\n        clip_output_dim = 768  # Assuming using ViT-Large with 768-dimensional embeddings\n        self.fc_features = nn.Linear(clip_output_dim + num_features, 512)\n        self.fc1 = nn.Linear(512, 256)\n\n        # Output layers for each of the 10 attributes (multi-label output)\n        self.attr_outputs = nn.ModuleList([nn.Linear(256, num_outputs) for num_outputs in num_outputs_per_attr])\n\n    def forward(self, inputs, features):\n        image_inputs = inputs['pixel_values'].squeeze(1).to(device)\n\n        # Forward pass through CLIP image encoder\n        with torch.no_grad():\n            image_embeddings = self.clip_model.get_image_features(image_inputs)\n\n        # Concatenate image embeddings with the additional features\n        x = torch.cat([image_embeddings, features], dim=1)\n\n        # Pass through the fully connected layers\n        x = torch.relu(self.fc_features(x))\n        x = torch.relu(self.fc1(x))\n\n        # Pass through the attribute-specific output layers (multi-label output)\n        outputs = [attr_output(x) for attr_output in self.attr_outputs]\n\n        return outputs\n\n# 5. Instantiate the model\nnum_features = 1 # 'Category' and 'len'\nnum_classes_list = [len(attr_encoders[f'attr_{i}'].classes_) for i in range(1, 11)]\nmodel = MultiOutputModel(clip_model, num_features=num_features, num_outputs_per_attr=num_classes_list).to(device)\n\n# 6. Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T09:59:50.778002Z","iopub.execute_input":"2024-11-04T09:59:50.778908Z","iopub.status.idle":"2024-11-04T09:59:50.818494Z","shell.execute_reply.started":"2024-11-04T09:59:50.778857Z","shell.execute_reply":"2024-11-04T09:59:50.817590Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\ndef load_model_checkpoint(model, checkpoint_path):\n    \"\"\"Load the model from the specified checkpoint path.\"\"\"\n    if os.path.exists(checkpoint_path):\n        state_dict = torch.load(checkpoint_path)\n        \n        # Modify keys for DataParallel\n        new_state_dict = {}\n        for k, v in state_dict.items():\n            new_state_dict[f'module.{k}'] = v\n            \n        model.load_state_dict(new_state_dict)\n        print(f\"Model loaded from {checkpoint_path}\")\n    else:\n        print(f\"Checkpoint not found at {checkpoint_path}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom tqdm import tqdm\n\ndef load_model_checkpoint(model, checkpoint_path):\n    \"\"\"Load the model from the specified checkpoint path.\"\"\"\n    if os.path.exists(checkpoint_path):\n        model.load_state_dict(torch.load(checkpoint_path))\n        print(f\"Model loaded from {checkpoint_path}\")\n    else:\n        print(f\"Checkpoint not found at {checkpoint_path}\")\n\ndef train_model(model, train_loader, criterion, optimizer, epochs=15, save_dir='model_checkpoints'):\n    os.makedirs(save_dir, exist_ok=True)\n    model.train()\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{epochs}', unit='batch') as pbar:\n            for batch_idx, ((inputs, features), labels) in enumerate(train_loader):\n                inputs = {k: v.to(device) for k, v in inputs.items()}\n                features, labels = features.to(device), labels.to(device)\n                optimizer.zero_grad()\n\n                # Forward pass\n                outputs = model(inputs, features)\n\n                total_loss = 0.0\n                for idx, (output, label) in enumerate(zip(outputs, labels.T)):\n                    loss = criterion(output, label)\n                    total_loss += loss\n\n                total_loss.backward()\n                optimizer.step()\n\n                running_loss += total_loss.item()\n                pbar.set_postfix({'Batch Loss': total_loss.item()})\n                pbar.update(1)\n\n        avg_epoch_loss = running_loss / len(train_loader)\n        print(f'Epoch {epoch + 1} completed. Average Loss: {avg_epoch_loss:.4f}')\n\n        # Save the model after each epoch\n        checkpoint_path = os.path.join(save_dir, f'model_epoch_clip_large_{epoch + 1}.pth')\n        torch.save(model.state_dict(), checkpoint_path)\n        print(f'Model saved at {checkpoint_path}')\n\n# Load model from checkpoint if specified\ncheckpoint_path = '/kaggle/input/newmods/model_checkpoints/model_epoch_clip_large_12.pth'\nload_model_checkpoint(model, checkpoint_path)\n\n# Example usage\ntrain_model(model, train_loader, criterion, optimizer, epochs=15)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T10:01:14.221853Z","iopub.execute_input":"2024-11-04T10:01:14.222790Z","iopub.status.idle":"2024-11-04T16:22:52.934845Z","shell.execute_reply.started":"2024-11-04T10:01:14.222743Z","shell.execute_reply":"2024-11-04T16:22:52.932907Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model loaded from /kaggle/input/newmods/model_checkpoints/model_epoch_clip_large_12.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/15: 100%|██████████| 1098/1098 [49:40<00:00,  2.71s/batch, Batch Loss=3.48]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 completed. Average Loss: 4.3323\nModel saved at model_checkpoints/model_epoch_clip_large_1.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/15: 100%|██████████| 1098/1098 [43:41<00:00,  2.39s/batch, Batch Loss=2.58]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 completed. Average Loss: 4.2641\nModel saved at model_checkpoints/model_epoch_clip_large_2.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/15: 100%|██████████| 1098/1098 [43:48<00:00,  2.39s/batch, Batch Loss=7.08]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 completed. Average Loss: 4.2224\nModel saved at model_checkpoints/model_epoch_clip_large_3.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/15: 100%|██████████| 1098/1098 [43:55<00:00,  2.40s/batch, Batch Loss=3.02]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 completed. Average Loss: 4.1806\nModel saved at model_checkpoints/model_epoch_clip_large_4.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/15: 100%|██████████| 1098/1098 [43:50<00:00,  2.40s/batch, Batch Loss=2.45]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 completed. Average Loss: 4.1399\nModel saved at model_checkpoints/model_epoch_clip_large_5.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/15: 100%|██████████| 1098/1098 [43:53<00:00,  2.40s/batch, Batch Loss=8.46]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 completed. Average Loss: 4.0879\nModel saved at model_checkpoints/model_epoch_clip_large_6.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/15: 100%|██████████| 1098/1098 [44:34<00:00,  2.44s/batch, Batch Loss=1.92]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 completed. Average Loss: 4.0617\nModel saved at model_checkpoints/model_epoch_clip_large_7.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/15: 100%|██████████| 1098/1098 [44:00<00:00,  2.41s/batch, Batch Loss=5.42]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 completed. Average Loss: 4.0325\nModel saved at model_checkpoints/model_epoch_clip_large_8.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/15:  54%|█████▎    | 590/1098 [23:39<20:22,  2.41s/batch, Batch Loss=4.09]\n\nKeyboardInterrupt\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nfrom transformers import CLIPProcessor, CLIPModel\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm  # Import tqdm for progress bar\n\ndef load_trained_model(model_path, clip_model, num_features, num_classes_list):\n    # Load the state dictionary from the model path\n    state_dict = torch.load(model_path)\n\n#     if list(state_dict.keys())[0].startswith(\"module.\"):\n#         # Remove 'module.' prefix if present\n#         state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n    \n    # Create the model instance before loading the state dictionary\n    model = MultiOutputModel(clip_model, num_features=num_features, num_outputs_per_attr=num_classes_list)\n    \n    # Load the state dictionary with strict=False to avoid key mismatches\n    model.load_state_dict(state_dict)\n    \n    model.to(device)\n    model.eval()  # Set to evaluation mode\n    return model\n\n\n# Paths and parameters\nmodel_path = '/kaggle/working/model_checkpoints/model_epoch_4.pth'  # Path to your trained model\nimage_dir = '/kaggle/input/mesho-chll/MESHO/test_images/'  # Path to test images directory\ncsv_file = '/kaggle/input/mesho-chll/MESHO/test.csv'  # Path to test CSV file\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load the CLIP model and processor\n# Load the CLIP model and processor\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n# Assuming the same encoding for the attributes used during training\nnum_features = 1  # 'Category'\nnum_classes_list = [len(attr_encoders[f'attr_{i}'].classes_) for i in range(1, 11)]\n\n# Load the model with the trained weights\nmodel = load_trained_model(model_path, clip_model, num_features, num_classes_list)\n\n# Load the test CSV that contains the 'id' and 'Category'\ntest_df = pd.read_csv(csv_file)\n\n# Preprocess a single image for inference\ndef preprocess_image(image_path, processor):\n    image = Image.open(image_path).convert('RGB')\n    inputs = processor(images=image, return_tensors=\"pt\")\n    return inputs\n\n# Inference function\ndef predict(model, image_path, processor, features):\n    # Preprocess the image\n    inputs = preprocess_image(image_path, processor)\n    inputs = inputs.to(device)\n\n    # Ensure features are properly shaped for model input\n    features = torch.tensor(features).unsqueeze(0).to(device)  # Shape as (1, num_features)\n\n    # Perform forward pass (inference)\n    with torch.no_grad():\n        attr_outputs = model(inputs, features)\n\n    # Convert outputs to predicted labels\n    predicted_labels = [torch.argmax(output, dim=1).item() for output in attr_outputs]\n   \n    return predicted_labels\n\n# Example: Perform inference on all test images\npredictions_list = []\n\n# Wrap the loop with tqdm for progress bar\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing Images\"):\n    image_id = str(row['id']).zfill(6) + '.jpg'  # Convert ID to format 000000.jpg\n    image_path = os.path.join(image_dir, image_id)\n\n    # Encode the Category (make sure it was encoded similarly as in training)\n    category_encoded_value = category_encoder.transform([row['Category']])[0]\n\n    # Perform prediction\n    predicted_attrs = predict(model, image_path, clip_processor, [category_encoded_value])\n\n    # Decode the predicted attributes back to their original labels\n    decoded_predictions = {f'attr_{i}': attr_encoders[f'attr_{i}'].inverse_transform([pred])[0] \n                           for i, pred in enumerate(predicted_attrs, 1)}\n    \n    # Store the results for this image, without Category for now\n    predictions_list.append({'id': row['id'], **decoded_predictions})\n\n# Convert predictions to a DataFrame for better output readability\npredictions_df = pd.DataFrame(predictions_list)\n\n# Merge predictions with the original test DataFrame based on 'id'\nmerged_df = pd.merge(test_df[['id', 'Category']], predictions_df, on='id')\n\n# Count attributes that are not predicted as 'no'\n# Assuming attribute columns are named attr_1, attr_2, ..., attr_10\nattribute_columns = [f'attr_{i}' for i in range(1, 11)]\n\n# Create the 'len' column based on the count of attributes that are not 'no'\nmerged_df['len'] = merged_df[attribute_columns].apply(lambda x: sum(attr != 'no' for attr in x), axis=1)\n\n# Reorder columns to have 'len' after 'Category' and before 'attr_1'\ncols = ['id', 'Category', 'len'] + attribute_columns\nmerged_df = merged_df[cols]\n\n# Display merged predictions for all test images\nprint(merged_df)\n\n# Save predictions to a CSV file\nmerged_df.to_csv('sub_vit_l-5.39.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T17:03:23.092762Z","iopub.execute_input":"2024-11-01T17:03:23.093148Z","iopub.status.idle":"2024-11-01T17:40:17.633278Z","shell.execute_reply.started":"2024-11-01T17:03:23.093112Z","shell.execute_reply":"2024-11-01T17:40:17.632295Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3563157646.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(model_path)\nProcessing Images: 100%|██████████| 30205/30205 [36:49<00:00, 13.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"          id             Category  len   attr_1  attr_2   attr_3      attr_4  \\\n0          0          Men Tshirts    1       no   round       no          no   \n1          1          Men Tshirts    3    white   round  printed          no   \n2          2          Men Tshirts    1       no   round       no          no   \n3          3          Men Tshirts    5  default    polo    solid       solid   \n4          4          Men Tshirts    1       no   round       no          no   \n...      ...                  ...  ...      ...     ...      ...         ...   \n30200  30484  Women Tops & Tunics    9    green    boxy     crop  round neck   \n30201  30485  Women Tops & Tunics    1       no      no       no          no   \n30202  30486  Women Tops & Tunics    1       no      no       no          no   \n30203  30487  Women Tops & Tunics    2       no      no  regular          no   \n30204  30488  Women Tops & Tunics    9     pink  fitted     crop     default   \n\n              attr_5   attr_6      attr_7         attr_8           attr_9  \\\n0                 no       no          no             no               no   \n1                 no       no          no             no               no   \n2                 no       no          no             no               no   \n3      short sleeves       no          no             no               no   \n4                 no       no          no             no               no   \n...              ...      ...         ...            ...              ...   \n30200         casual  printed  typography  short sleeves  regular sleeves   \n30201             no       no          no             no     puff sleeves   \n30202             no       no          no             no          default   \n30203             no       no          no   long sleeves               no   \n30204         casual    solid       solid     sleeveless          default   \n\n      attr_10  \n0          no  \n1          no  \n2          no  \n3          no  \n4          no  \n...       ...  \n30200      no  \n30201      no  \n30202      no  \n30203      no  \n30204      no  \n\n[30205 rows x 13 columns]\n","output_type":"stream"}]}]}