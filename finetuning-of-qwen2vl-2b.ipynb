{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"01bb08ee771d4d9cb28eb3d3f8761097":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04944bc399284b7780ea646e85439f65":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05c48a17d76144bda757daf43351793c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0abca11d2558420189fc5b4523ad98da":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e21ed297ee2450db2780cc4e7e46090":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"116a869317724427ad14b57a2ced1410":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_27112dce70654db186647833d4326807","placeholder":"​","style":"IPY_MODEL_0e21ed297ee2450db2780cc4e7e46090","value":"Login successful"}},"155e724aa0bb49839df0470348f3121d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95ff8229ab7b430b9961cdeb9e3bf937","placeholder":"​","style":"IPY_MODEL_c8d0021edddd4373ad4f1fd13ccb9138","value":"Your token has been saved in your configured git credential helpers (store)."}},"1ca6756088bc48dc99bd4f1fde7e0736":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e67b165c09344bc193eea3e9489303a3","placeholder":"​","style":"IPY_MODEL_f78639236e2042eaad3d97a042478627","value":" 1.99G/1.99G [01:24&lt;00:00, 28.2MB/s]"}},"27112dce70654db186647833d4326807":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27801421973f4717ab6b768416332beb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c447d59766c49129e54e11b858866a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f55868b24fa4aaaba5b3ec2228377a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0abca11d2558420189fc5b4523ad98da","placeholder":"​","style":"IPY_MODEL_e6c0de99b8be4a06ab3c9b1cd3b34c05","value":"model-00003-of-00003.safetensors: 100%"}},"32c207aa859a426a843fc848fe3eda96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34c41c2b13624097a54e04e96719b11e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4444f494851f49628f5b13d69e4a0ab9","IPY_MODEL_b100d3deb05a4e8080822964b325daac","IPY_MODEL_8fc80b703c734644b347c64cfc5f242d"],"layout":"IPY_MODEL_816a983cc5cb499186dbce7d63545f0c"}},"37ac9b0ebe3d4796960e9fca9d78b75c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f62ee14663844b3cba0c54d18079a9dd","max":1995539928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ce660801eb964a949aa6e45e1e37f3b8","value":1995539928}},"3b278d691d434dab91b6f8e4f43c8e2f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4444f494851f49628f5b13d69e4a0ab9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01bb08ee771d4d9cb28eb3d3f8761097","placeholder":"​","style":"IPY_MODEL_27801421973f4717ab6b768416332beb","value":"Upload 3 LFS files: 100%"}},"4552bfe518684ed0a04433959d1b995c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05c48a17d76144bda757daf43351793c","placeholder":"​","style":"IPY_MODEL_f647a39f877d455ba418dd5e87dda5c1","value":"Token is valid (permission: write)."}},"4d8067a77c724a3aa489a82e6944f837":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f55868b24fa4aaaba5b3ec2228377a9","IPY_MODEL_df181985eb0c4d6c8ffb93da5ef73b33","IPY_MODEL_9ab290a4c6614880a52a2e6b28eb9369"],"layout":"IPY_MODEL_84407ce766db457aa67ad40e38076ef1"}},"5419752e153c4f08b47eadcee5d7667d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55da799da5604cb88decfc813885da71":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a49ab00cf784db791a4f470ae12985b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62503435e2254257b3769deb6842386a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64687bc58bde443091796d1c33239b9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64cdb1e5b49c475097fb3ccd827ef53c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ceaf0959bae49018d0aa5cd8890a67b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e739257d87c4719b230a6c9c17077d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"816a983cc5cb499186dbce7d63545f0c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84407ce766db457aa67ad40e38076ef1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e21f07c147a40f6863da8dae844d289":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8eb18e6127ad4018a44b50b0d905ed24":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_967e49d2ad28426e9844c3963429843a","placeholder":"​","style":"IPY_MODEL_3b278d691d434dab91b6f8e4f43c8e2f","value":" 2.00G/2.00G [01:24&lt;00:00, 30.1MB/s]"}},"8f31eabed62c4eec852be928c1b17beb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fc80b703c734644b347c64cfc5f242d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bc84fae80a845d7bdda2749d20d397d","placeholder":"​","style":"IPY_MODEL_2c447d59766c49129e54e11b858866a5","value":" 3/3 [01:25&lt;00:00, 85.22s/it]"}},"95ff8229ab7b430b9961cdeb9e3bf937":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"967e49d2ad28426e9844c3963429843a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98bba0e196a04d57b7c9d6bd762eb9b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ab290a4c6614880a52a2e6b28eb9369":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5419752e153c4f08b47eadcee5d7667d","placeholder":"​","style":"IPY_MODEL_64687bc58bde443091796d1c33239b9e","value":" 429M/429M [00:22&lt;00:00, 28.5MB/s]"}},"9bc84fae80a845d7bdda2749d20d397d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b02a5ced9fc847269f36f0019915b8c0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b100d3deb05a4e8080822964b325daac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f31eabed62c4eec852be928c1b17beb","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_62503435e2254257b3769deb6842386a","value":3}},"b44f4b0e40024d16b1ff9c0780e970fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c8578aa0764246e489b3e58e69b11d0e","IPY_MODEL_c96f5c6c3a7b4612987aa2eac7400a8c","IPY_MODEL_1ca6756088bc48dc99bd4f1fde7e0736"],"layout":"IPY_MODEL_de40e74720ec46e7829ed8a4405ebf4e"}},"be22f4d810e748f380b19ad5c8810c0f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8578aa0764246e489b3e58e69b11d0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04944bc399284b7780ea646e85439f65","placeholder":"​","style":"IPY_MODEL_8e21f07c147a40f6863da8dae844d289","value":"model-00002-of-00003.safetensors: 100%"}},"c8d0021edddd4373ad4f1fd13ccb9138":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c96f5c6c3a7b4612987aa2eac7400a8c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b02a5ced9fc847269f36f0019915b8c0","max":1993062424,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ceaf0959bae49018d0aa5cd8890a67b","value":1993062424}},"ce660801eb964a949aa6e45e1e37f3b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d35ae27cd4384449a85bd06f505f79b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_4552bfe518684ed0a04433959d1b995c","IPY_MODEL_155e724aa0bb49839df0470348f3121d","IPY_MODEL_e50f25d74d414c79bd9593405fb4ecd3","IPY_MODEL_116a869317724427ad14b57a2ced1410"],"layout":"IPY_MODEL_7e739257d87c4719b230a6c9c17077d3"}},"de40e74720ec46e7829ed8a4405ebf4e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df181985eb0c4d6c8ffb93da5ef73b33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_55da799da5604cb88decfc813885da71","max":429448016,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5a49ab00cf784db791a4f470ae12985b","value":429448016}},"e50f25d74d414c79bd9593405fb4ecd3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcff34f37a22473e998a632a7a11c946","placeholder":"​","style":"IPY_MODEL_64cdb1e5b49c475097fb3ccd827ef53c","value":"Your token has been saved to /root/.cache/huggingface/token"}},"e67b165c09344bc193eea3e9489303a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6c0de99b8be4a06ab3c9b1cd3b34c05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec2e508bc5774d979d265edff6fff9f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be22f4d810e748f380b19ad5c8810c0f","placeholder":"​","style":"IPY_MODEL_98bba0e196a04d57b7c9d6bd762eb9b7","value":"model-00001-of-00003.safetensors: 100%"}},"f62ee14663844b3cba0c54d18079a9dd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f647a39f877d455ba418dd5e87dda5c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f78639236e2042eaad3d97a042478627":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc98333aa5744fd4b55787f075035ec1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ec2e508bc5774d979d265edff6fff9f5","IPY_MODEL_37ac9b0ebe3d4796960e9fca9d78b75c","IPY_MODEL_8eb18e6127ad4018a44b50b0d905ed24"],"layout":"IPY_MODEL_32c207aa859a426a843fc848fe3eda96"}},"fcff34f37a22473e998a632a7a11c946":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9704269,"sourceType":"datasetVersion","datasetId":5934849},{"sourceId":9878449,"sourceType":"datasetVersion","datasetId":6064921},{"sourceId":9887446,"sourceType":"datasetVersion","datasetId":6072006},{"sourceId":9901611,"sourceType":"datasetVersion","datasetId":6082540},{"sourceId":9907680,"sourceType":"datasetVersion","datasetId":6087158}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/hiyouga/LLaMA-Factory.git\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X3Anl3QGdZO_","outputId":"891d6ee2-706c-4a6b-c107-3a1cd2e0cf3d","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:27:42.381375Z","iopub.execute_input":"2024-11-16T07:27:42.381719Z","iopub.status.idle":"2024-11-16T07:27:55.985669Z","shell.execute_reply.started":"2024-11-16T07:27:42.381683Z","shell.execute_reply":"2024-11-16T07:27:55.984584Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'LLaMA-Factory'...\nremote: Enumerating objects: 18916, done.\u001b[K\nremote: Counting objects: 100% (954/954), done.\u001b[K\nremote: Compressing objects: 100% (377/377), done.\u001b[K\nremote: Total 18916 (delta 596), reused 804 (delta 569), pack-reused 17962 (from 1)\u001b[K\nReceiving objects: 100% (18916/18916), 231.83 MiB | 26.82 MiB/s, done.\nResolving deltas: 100% (13773/13773), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%cd LLaMA-Factory","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Wxs3qbI6eiDN","outputId":"6683ced7-085e-4368-ff00-dd48594b8687","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:27:55.987819Z","iopub.execute_input":"2024-11-16T07:27:55.988162Z","iopub.status.idle":"2024-11-16T07:27:55.994763Z","shell.execute_reply.started":"2024-11-16T07:27:55.988125Z","shell.execute_reply":"2024-11-16T07:27:55.993844Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/LLaMA-Factory\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pwd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qLkQ_Txve5_H","outputId":"c7972c82-694e-458d-8f9d-65dabc52264d","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:27:55.995881Z","iopub.execute_input":"2024-11-16T07:27:55.996166Z","iopub.status.idle":"2024-11-16T07:27:56.975820Z","shell.execute_reply.started":"2024-11-16T07:27:55.996136Z","shell.execute_reply":"2024-11-16T07:27:56.974930Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/LLaMA-Factory\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d9ZxPCp3e9vk","outputId":"55425c6d-6eec-4be9-d5a1-a2002519afc0","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:27:56.977250Z","iopub.execute_input":"2024-11-16T07:27:56.977603Z","iopub.status.idle":"2024-11-16T07:28:20.617460Z","shell.execute_reply.started":"2024-11-16T07:27:56.977549Z","shell.execute_reply":"2024-11-16T07:28:20.616465Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers<=4.46.1,>=4.41.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (4.45.1)\nRequirement already satisfied: datasets<=3.1.0,>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.0.1)\nRequirement already satisfied: accelerate<=1.0.1,>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.34.2)\nCollecting peft<=0.12.0,>=0.11.1 (from -r requirements.txt (line 4))\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nCollecting trl<=0.9.6,>=0.8.6 (from -r requirements.txt (line 5))\n  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\nCollecting gradio<5.0.0,>=4.0.0 (from -r requirements.txt (line 6))\n  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.14.1)\nCollecting einops (from -r requirements.txt (line 9))\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.2.0)\nCollecting tiktoken (from -r requirements.txt (line 11))\n  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (3.20.3)\nRequirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (0.30.1)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (2.9.2)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (0.111.0)\nCollecting sse-starlette (from -r requirements.txt (line 16))\n  Downloading sse_starlette-2.1.3-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (3.7.5)\nCollecting fire (from -r requirements.txt (line 18))\n  Downloading fire-0.7.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (21.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (6.0.2)\nRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (1.26.4)\nCollecting av (from -r requirements.txt (line 22))\n  Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (0.25.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (0.3.8)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (3.9.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (2.4.0)\nCollecting tyro>=0.5.11 (from trl<=0.9.6,>=0.8.6->-r requirements.txt (line 5))\n  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (4.4.0)\nCollecting ffmpy (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6))\n  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\nCollecting gradio-client==1.3.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6))\n  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.27.0)\nRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (6.4.0)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (3.1.4)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (2.1.5)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (3.10.4)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (10.3.0)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.25.1)\nRequirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.0.9)\nCollecting ruff>=0.2.2 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6))\n  Downloading ruff-0.7.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting semantic-version~=2.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6))\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting tomlkit==0.12.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6))\n  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.12.3)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (4.12.2)\nCollecting urllib3~=2.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6))\n  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: websockets<13.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (12.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2024.1)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 13)) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 13)) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 14)) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 14)) (2.23.4)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 15)) (0.37.2)\nRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 15)) (0.0.4)\nRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 15)) (5.10.0)\nRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 15)) (2.1.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (3.1.2)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->-r requirements.txt (line 18)) (2.4.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (3.7)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (1.3.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (1.2.0)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 15)) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (4.0.3)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (1.0.5)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 7)) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (3.3.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (3.3)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (13.7.1)\nRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->-r requirements.txt (line 5)) (0.16)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->-r requirements.txt (line 5))\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 15)) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 15)) (1.0.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 15)) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 15)) (0.22.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (2.18.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.1.2)\nDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\nDownloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ruff-0.7.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\nDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114248 sha256=1cfbc998281dbcaec34ca58e34d89f7d7537099a72b9afcfbd01a3af7d8d29ff\n  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\nSuccessfully built fire\nInstalling collected packages: urllib3, tomlkit, shtab, semantic-version, ruff, fire, ffmpy, einops, av, tyro, tiktoken, sse-starlette, gradio-client, trl, peft, gradio\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.18\n    Uninstalling urllib3-1.26.18:\n      Successfully uninstalled urllib3-1.26.18\n  Attempting uninstall: tomlkit\n    Found existing installation: tomlkit 0.13.2\n    Uninstalling tomlkit-0.13.2:\n      Successfully uninstalled tomlkit-0.13.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.3 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed av-13.1.0 einops-0.8.0 ffmpy-0.4.0 fire-0.7.0 gradio-4.44.1 gradio-client-1.3.0 peft-0.12.0 ruff-0.7.4 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.1.3 tiktoken-0.8.0 tomlkit-0.12.0 trl-0.9.6 tyro-0.8.14 urllib3-2.2.1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install bitsandbytes","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N_-p_QzCfGPv","outputId":"fc10571c-fcd3-41b4-c34a-e02019ac45a0","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:28:20.620129Z","iopub.execute_input":"2024-11-16T07:28:20.620473Z","iopub.status.idle":"2024-11-16T07:28:36.742928Z","shell.execute_reply.started":"2024-11-16T07:28:20.620436Z","shell.execute_reply":"2024-11-16T07:28:36.741959Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y7dKDy_lfOHg","outputId":"fe0fb4c9-8660-4540-ca36-474e6a8ff45b","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:28:36.744300Z","iopub.execute_input":"2024-11-16T07:28:36.744629Z","iopub.status.idle":"2024-11-16T07:29:26.632800Z","shell.execute_reply.started":"2024-11-16T07:28:36.744593Z","shell.execute_reply":"2024-11-16T07:29:26.631654Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/transformers.git\n  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-jalljggx\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-jalljggx\n  Resolved https://github.com/huggingface/transformers.git to commit 13493215abceafc1653af88b045120014fb4c1fc\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (0.20.0)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.0.dev0) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.0.dev0) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.0.dev0) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.47.0.dev0) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47.0.dev0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47.0.dev0) (2.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47.0.dev0) (2024.8.30)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.47.0.dev0-py3-none-any.whl size=10052486 sha256=c5e8175efa7cc1c68b16a0c2f52d4aa059808f8af27a568660d4c25fa5e2c1ce\n  Stored in directory: /tmp/pip-ephem-wheel-cache-72q_dwi2/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed transformers-4.47.0.dev0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install -e \".[torch, metrics]\"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-xHHHkVtfdL4","outputId":"7b6ccfa4-4e10-495a-acd3-48b01c30a288","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:29:26.634328Z","iopub.execute_input":"2024-11-16T07:29:26.634752Z","iopub.status.idle":"2024-11-16T07:30:01.865081Z","shell.execute_reply.started":"2024-11-16T07:29:26.634705Z","shell.execute_reply":"2024-11-16T07:30:01.864093Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Obtaining file:///kaggle/working/LLaMA-Factory\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting transformers<=4.46.1,>=4.41.2 (from llamafactory==0.9.1.dev0)\n  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets<=3.1.0,>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (3.0.1)\nRequirement already satisfied: accelerate<=1.0.1,>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.34.2)\nRequirement already satisfied: peft<=0.12.0,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.12.0)\nRequirement already satisfied: trl<=0.9.6,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.9.6)\nRequirement already satisfied: gradio<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (4.44.1)\nRequirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (2.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (1.14.1)\nRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.8.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.2.0)\nRequirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.8.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (3.20.3)\nRequirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.30.1)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (2.9.2)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.111.0)\nRequirement already satisfied: sse-starlette in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (2.1.3)\nRequirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (3.7.5)\nRequirement already satisfied: fire in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.7.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (21.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (6.0.2)\nRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (1.26.4)\nRequirement already satisfied: av in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (13.1.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (3.2.4)\nRequirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.42.1)\nCollecting rouge-chinese (from llamafactory==0.9.1.dev0)\n  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: torch>=1.13.1 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (2.4.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.1.dev0) (5.9.3)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.1.dev0) (0.25.1)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.1.dev0) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (0.3.8)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.9.5)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (4.4.0)\nRequirement already satisfied: ffmpy in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.4.0)\nRequirement already satisfied: gradio-client==1.3.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.3.0)\nRequirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.27.0)\nRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (6.4.0)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (3.1.4)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2.1.5)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (3.10.4)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (10.3.0)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.25.1)\nRequirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.0.9)\nRequirement already satisfied: ruff>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.7.4)\nRequirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2.10.0)\nRequirement already satisfied: tomlkit==0.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.12.0)\nRequirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.12.3)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (4.12.2)\nRequirement already satisfied: urllib3~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2.2.1)\nRequirement already satisfied: websockets<13.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (12.0)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->llamafactory==0.9.1.dev0) (0.37.2)\nRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->llamafactory==0.9.1.dev0) (0.0.4)\nRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi->llamafactory==0.9.1.dev0) (5.10.0)\nRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->llamafactory==0.9.1.dev0) (2.1.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->llamafactory==0.9.1.dev0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic->llamafactory==0.9.1.dev0) (2.23.4)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (3.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->llamafactory==0.9.1.dev0) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->llamafactory==0.9.1.dev0) (0.20.0)\nRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (0.8.14)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->llamafactory==0.9.1.dev0) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn->llamafactory==0.9.1.dev0) (0.14.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->llamafactory==0.9.1.dev0) (2.4.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->llamafactory==0.9.1.dev0) (1.16.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (3.7)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.3.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.2.0)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->llamafactory==0.9.1.dev0) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.0.3)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.0.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.3.2)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (13.7.1)\nRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (0.16)\nRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (1.7.1)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.9.1.dev0) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.9.1.dev0) (1.0.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.9.1.dev0) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.9.1.dev0) (0.22.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.1->llamafactory==0.9.1.dev0) (1.3.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.1.2)\nDownloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\nChecking if build backend supports build_editable ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: llamafactory\n  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.1.dev0-0.editable-py3-none-any.whl size=23007 sha256=6fbef9a17a47dd0319203c80c8643af8ffb8f0cf68a62f15539131a04c96a1a9\n  Stored in directory: /tmp/pip-ephem-wheel-cache-zlqnwe_h/wheels/21/5a/a2/9a8fea19e68e32089e22401d08554f51119f2464cad3a126ec\nSuccessfully built llamafactory\nInstalling collected packages: rouge-chinese, transformers, llamafactory\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0.dev0\n    Uninstalling transformers-4.47.0.dev0:\n      Successfully uninstalled transformers-4.47.0.dev0\nSuccessfully installed llamafactory-0.9.1.dev0 rouge-chinese-1.0.3 transformers-4.46.1\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install liger-kernel","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C5TaOw-5fqIZ","outputId":"6f515ee9-2230-487d-92b5-25e2bb3a452f","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:30:01.866538Z","iopub.execute_input":"2024-11-16T07:30:01.866871Z","iopub.status.idle":"2024-11-16T07:30:20.660046Z","shell.execute_reply.started":"2024-11-16T07:30:01.866835Z","shell.execute_reply":"2024-11-16T07:30:20.659107Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting liger-kernel\n  Downloading liger_kernel-0.4.1-py3-none-any.whl.metadata (28 kB)\nRequirement already satisfied: torch>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from liger-kernel) (2.4.0)\nCollecting triton>=2.3.1 (from liger-kernel)\n  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.1.2->liger-kernel) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.1.2->liger-kernel) (1.3.0)\nDownloading liger_kernel-0.4.1-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, liger-kernel\nSuccessfully installed liger-kernel-0.4.1 triton-3.1.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import csv\nimport json\nimport os\n\n# Paths\ncsv_file_path = '/kaggle/input/all-cat-meesh/cat3_new.csv'  # Path to your CSV file\nimage_folder_path = '/kaggle/input/meesho/MESHO/train_images'  # Folder where your images are stored\noutput_dir = '/kaggle/working/LLaMA-Factory/data'  # Directory to save the JSON files\noutput_json_file = os.path.join(output_dir, 'cat3.json')  # Output JSON file\ndataset_info_file = os.path.join(output_dir, 'dataset_info.json')  # Dataset info JSON file\n\n# Create the 'data' directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Define system message and prompt structure\nsystem_message = \"\"\"You are a highly knowledgeable AI assistant specialized in visual recognition and attribute prediction. Your role is to analyze the provided product image alongside its category information. Using examples of previously provided attributes, predict key product characteristics that align with the product's visual features. Respond with the values for attr_1 through attr_10, without any additional explanations.\"\"\"\n\nprompt_template = \"\"\"Given the provided product category (##CATEGORY##) and its corresponding image, please predict the following attributes for the product: attr_1 to attr_10 based on previously observed patterns and examples. Ensure that your predictions are accurate and relevant to the visual characteristics in the image.\"\"\"\n\n# Initialize list to store the dataset\ndataset = []\n\n# Open and read the CSV file\nwith open(csv_file_path, 'r') as csv_file:\n    reader = csv.DictReader(csv_file)\n    \n    for row in reader:\n        # Generate padded image ID to match filename\n        image_id = str(row[\"id\"]).zfill(6)\n        image_filename = f\"{image_id}.jpg\"\n        \n        # Construct the image path in your train folder\n        image_path = os.path.join(image_folder_path, image_filename)\n\n        # Ensure the image exists in the train folder before adding to the dataset\n        if os.path.exists(image_path):\n            # Construct human instruction with category information\n            human_instruction = prompt_template.replace(\"##CATEGORY##\", row[\"Category\"])\n            \n            # Construct the conversation structure as required\n            conversation = {\n                \"conversations\": [\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"<image>{human_instruction}\"\n                    },\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": json.dumps({f\"attr_{i}\": row.get(f\"attr_{i}\", \"\") for i in range(1, 11)})\n                    }\n                ],\n                \"images\": [image_path]\n            }\n\n            # Add the conversation to the dataset\n            dataset.append(conversation)\n        else:\n            print(f\"Image {image_filename} not found in {image_folder_path}, skipping.\")\n\n# Save the dataset to a JSON file\nwith open(output_json_file, 'w') as json_file:\n    json.dump(dataset, json_file, indent=4)\n\nprint(f\"Dataset successfully saved to {output_json_file}\")\n\n# Load existing dataset_info.json or create an empty dictionary if it doesn't exist\nif os.path.exists(dataset_info_file):\n    with open(dataset_info_file, 'r') as json_file:\n        dataset_info = json.load(json_file)\nelse:\n    dataset_info = {}\n\n# Add train_cat4 information to dataset_info\ndataset_info[\"cat3\"] = {\n    \"file_name\": \"cat3.json\",\n    \"formatting\": \"sharegpt\",\n    \"columns\": {\n        \"messages\": \"conversations\",\n        \"images\": \"images\"\n    },\n    \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n    }\n}\n\n# Save the updated dataset_info.json\nwith open(dataset_info_file, 'w') as json_file:\n    json.dump(dataset_info, json_file, indent=4)\n\nprint(f\"Dataset info successfully updated and saved to {dataset_info_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:30:20.661542Z","iopub.execute_input":"2024-11-16T07:30:20.661888Z","iopub.status.idle":"2024-11-16T07:30:45.888298Z","shell.execute_reply.started":"2024-11-16T07:30:20.661851Z","shell.execute_reply":"2024-11-16T07:30:45.887375Z"}},"outputs":[{"name":"stdout","text":"Dataset successfully saved to /kaggle/working/LLaMA-Factory/data/cat3.json\nDataset info successfully updated and saved to /kaggle/working/LLaMA-Factory/data/dataset_info.json\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import csv\nimport json\nimport os\n\n# Paths\ncsv_file_path = '/kaggle/input/cat2-new-mdified/cat_2_new_modified.csv'  # Path to your CSV file\nimage_folder_path = '/kaggle/input/meesho/MESHO/train_images'  # Folder where your images are stored\noutput_dir = '/kaggle/working/LLaMA-Factory/data'  # Directory to save the JSON files\noutput_json_file = os.path.join(output_dir, 'cat2.json')  # Output JSON file\ndataset_info_file = os.path.join(output_dir, 'dataset_info.json')  # Dataset info JSON file\n\n# Create the 'data' directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Define system message and prompt structure\nsystem_message = \"\"\"You are a highly knowledgeable AI assistant specialized in visual recognition and attribute prediction. Your role is to analyze the provided product image alongside its category information. Using examples of previously provided attributes, predict key product characteristics that align with the product's visual features. Respond with the values for attr_1 through attr_10, without any additional explanations.\"\"\"\n\nprompt_template = \"\"\"Given the provided product category (##CATEGORY##) and its corresponding image, please predict the following attributes for the product: attr_1 to attr_10 based on previously observed patterns and examples. Ensure that your predictions are accurate and relevant to the visual characteristics in the image.\"\"\"\n\n# Initialize list to store the dataset\ndataset = []\n\n# Open and read the CSV file\nwith open(csv_file_path, 'r') as csv_file:\n    reader = csv.DictReader(csv_file)\n    \n    for row in reader:\n        # Generate padded image ID to match filename\n        image_id = str(row[\"id\"]).zfill(6)\n        image_filename = f\"{image_id}.jpg\"\n        \n        # Construct the image path in your train folder\n        image_path = os.path.join(image_folder_path, image_filename)\n\n        # Ensure the image exists in the train folder before adding to the dataset\n        if os.path.exists(image_path):\n            # Construct human instruction with category information\n            human_instruction = prompt_template.replace(\"##CATEGORY##\", row[\"Category\"])\n            \n            # Construct the conversation structure as required\n            conversation = {\n                \"conversations\": [\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"<image>{human_instruction}\"\n                    },\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": json.dumps({f\"attr_{i}\": row.get(f\"attr_{i}\", \"\") for i in range(1, 11)})\n                    }\n                ],\n                \"images\": [image_path]\n            }\n\n            # Add the conversation to the dataset\n            dataset.append(conversation)\n        else:\n            print(f\"Image {image_filename} not found in {image_folder_path}, skipping.\")\n\n# Save the dataset to a JSON file\nwith open(output_json_file, 'w') as json_file:\n    json.dump(dataset, json_file, indent=4)\n\nprint(f\"Dataset successfully saved to {output_json_file}\")\n\n# Load existing dataset_info.json or create an empty dictionary if it doesn't exist\nif os.path.exists(dataset_info_file):\n    with open(dataset_info_file, 'r') as json_file:\n        dataset_info = json.load(json_file)\nelse:\n    dataset_info = {}\n\n# Add train_cat4 information to dataset_info\ndataset_info[\"cat2\"] = {\n    \"file_name\": \"cat2.json\",\n    \"formatting\": \"sharegpt\",\n    \"columns\": {\n        \"messages\": \"conversations\",\n        \"images\": \"images\"\n    },\n    \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n    }\n}\n\n# Save the updated dataset_info.json\nwith open(dataset_info_file, 'w') as json_file:\n    json.dump(dataset_info, json_file, indent=4)\n\nprint(f\"Dataset info successfully updated and saved to {dataset_info_file}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv\nimport json\nimport os\n\n# Paths\ncsv_file_path = '/kaggle/input/all-cat-meesh/cat3_new.csv'  # Path to your CSV file\nimage_folder_path = '/kaggle/input/meesho/MESHO/train_images'  # Folder where your images are stored\noutput_dir = '/kaggle/working/LLaMA-Factory/data'  # Directory to save the JSON files\noutput_json_file = os.path.join(output_dir, 'cat3.json')  # Output JSON file\ndataset_info_file = os.path.join(output_dir, 'dataset_info.json')  # Dataset info JSON file\n\n# Create the 'data' directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Define system message and prompt structure\nsystem_message = \"\"\"You are a highly knowledgeable AI assistant specialized in visual recognition and attribute prediction. Your role is to analyze the provided product image alongside its category information. Using examples of previously provided attributes, predict key product characteristics that align with the product's visual features. Respond with the values for attr_1 through attr_10, without any additional explanations.\"\"\"\n\nprompt_template = \"\"\"Given the provided product category (##CATEGORY##) and its corresponding image, please predict the following attributes for the product: attr_1 to attr_10 based on previously observed patterns and examples. Ensure that your predictions are accurate and relevant to the visual characteristics in the image.\"\"\"\n\n# Initialize list to store the dataset\ndataset = []\n\n# Open and read the CSV file\nwith open(csv_file_path, 'r') as csv_file:\n    reader = csv.DictReader(csv_file)\n    \n    for row in reader:\n        # Generate padded image ID to match filename\n        image_id = str(row[\"id\"]).zfill(6)\n        image_filename = f\"{image_id}.jpg\"\n        \n        # Construct the image path in your train folder\n        image_path = os.path.join(image_folder_path, image_filename)\n\n        # Ensure the image exists in the train folder before adding to the dataset\n        if os.path.exists(image_path):\n            # Construct human instruction with category information\n            human_instruction = prompt_template.replace(\"##CATEGORY##\", row[\"Category\"])\n            \n            # Construct the conversation structure as required\n            conversation = {\n                \"conversations\": [\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"<image>{human_instruction}\"\n                    },\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": json.dumps({f\"attr_{i}\": row.get(f\"attr_{i}\", \"\") for i in range(1, 11)})\n                    }\n                ],\n                \"images\": [image_path]\n            }\n\n            # Add the conversation to the dataset\n            dataset.append(conversation)\n        else:\n            print(f\"Image {image_filename} not found in {image_folder_path}, skipping.\")\n\n# Save the dataset to a JSON file\nwith open(output_json_file, 'w') as json_file:\n    json.dump(dataset, json_file, indent=4)\n\nprint(f\"Dataset successfully saved to {output_json_file}\")\n\n# Load existing dataset_info.json or create an empty dictionary if it doesn't exist\nif os.path.exists(dataset_info_file):\n    with open(dataset_info_file, 'r') as json_file:\n        dataset_info = json.load(json_file)\nelse:\n    dataset_info = {}\n\n# Add train_cat4 information to dataset_info\ndataset_info[\"cat3\"] = {\n    \"file_name\": \"cat3.json\",\n    \"formatting\": \"sharegpt\",\n    \"columns\": {\n        \"messages\": \"conversations\",\n        \"images\": \"images\"\n    },\n    \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n    }\n}\n\n# Save the updated dataset_info.json\nwith open(dataset_info_file, 'w') as json_file:\n    json.dump(dataset_info, json_file, indent=4)\n\nprint(f\"Dataset info successfully updated and saved to {dataset_info_file}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv\nimport json\nimport os\n\n# Paths\ncsv_file_path = '/kaggle/input/all-cat-meesh/cat4_new.csv'  # Path to your CSV file\nimage_folder_path = '/kaggle/input/meesho/MESHO/train_images'  # Folder where your images are stored\noutput_dir = '/kaggle/working/LLaMA-Factory/data'  # Directory to save the JSON files\noutput_json_file = os.path.join(output_dir, 'cat4.json')  # Output JSON file\ndataset_info_file = os.path.join(output_dir, 'dataset_info.json')  # Dataset info JSON file\n\n# Create the 'data' directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Define system message and prompt structure\nsystem_message = \"\"\"You are a highly knowledgeable AI assistant specialized in visual recognition and attribute prediction. Your role is to analyze the provided product image alongside its category information. Using examples of previously provided attributes, predict key product characteristics that align with the product's visual features. Respond with the values for attr_1 through attr_10, without any additional explanations.\"\"\"\n\nprompt_template = \"\"\"Given the provided product category (##CATEGORY##) and its corresponding image, please predict the following attributes for the product: attr_1 to attr_10 based on previously observed patterns and examples. Ensure that your predictions are accurate and relevant to the visual characteristics in the image.\"\"\"\n\n# Initialize list to store the dataset\ndataset = []\n\n# Open and read the CSV file\nwith open(csv_file_path, 'r') as csv_file:\n    reader = csv.DictReader(csv_file)\n    \n    for row in reader:\n        # Generate padded image ID to match filename\n        image_id = str(row[\"id\"]).zfill(6)\n        image_filename = f\"{image_id}.jpg\"\n        \n        # Construct the image path in your train folder\n        image_path = os.path.join(image_folder_path, image_filename)\n\n        # Ensure the image exists in the train folder before adding to the dataset\n        if os.path.exists(image_path):\n            # Construct human instruction with category information\n            human_instruction = prompt_template.replace(\"##CATEGORY##\", row[\"Category\"])\n            \n            # Construct the conversation structure as required\n            conversation = {\n                \"conversations\": [\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"<image>{human_instruction}\"\n                    },\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": json.dumps({f\"attr_{i}\": row.get(f\"attr_{i}\", \"\") for i in range(1, 11)})\n                    }\n                ],\n                \"images\": [image_path]\n            }\n\n            # Add the conversation to the dataset\n            dataset.append(conversation)\n        else:\n            print(f\"Image {image_filename} not found in {image_folder_path}, skipping.\")\n\n# Save the dataset to a JSON file\nwith open(output_json_file, 'w') as json_file:\n    json.dump(dataset, json_file, indent=4)\n\nprint(f\"Dataset successfully saved to {output_json_file}\")\n\n# Load existing dataset_info.json or create an empty dictionary if it doesn't exist\nif os.path.exists(dataset_info_file):\n    with open(dataset_info_file, 'r') as json_file:\n        dataset_info = json.load(json_file)\nelse:\n    dataset_info = {}\n\n# Add train_cat4 information to dataset_info\ndataset_info[\"cat4\"] = {\n    \"file_name\": \"cat4.json\",\n    \"formatting\": \"sharegpt\",\n    \"columns\": {\n        \"messages\": \"conversations\",\n        \"images\": \"images\"\n    },\n    \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n    }\n}\n\n# Save the updated dataset_info.json\nwith open(dataset_info_file, 'w') as json_file:\n    json.dump(dataset_info, json_file, indent=4)\n\nprint(f\"Dataset info successfully updated and saved to {dataset_info_file}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv\nimport json\nimport os\n\n# Paths\ncsv_file_path = '/kaggle/input/all-cat-meesh/cat5_new.csv'  # Path to your CSV file\nimage_folder_path = '/kaggle/input/meesho/MESHO/train_images'  # Folder where your images are stored\noutput_dir = '/kaggle/working/LLaMA-Factory/data'  # Directory to save the JSON files\noutput_json_file = os.path.join(output_dir, 'cat5.json')  # Output JSON file\ndataset_info_file = os.path.join(output_dir, 'dataset_info.json')  # Dataset info JSON file\n\n# Create the 'data' directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Define system message and prompt structure\nsystem_message = \"\"\"You are a highly knowledgeable AI assistant specialized in visual recognition and attribute prediction. Your role is to analyze the provided product image alongside its category information. Using examples of previously provided attributes, predict key product characteristics that align with the product's visual features. Respond with the values for attr_1 through attr_10, without any additional explanations.\"\"\"\n\nprompt_template = \"\"\"Given the provided product category (##CATEGORY##) and its corresponding image, please predict the following attributes for the product: attr_1 to attr_10 based on previously observed patterns and examples. Ensure that your predictions are accurate and relevant to the visual characteristics in the image.\"\"\"\n\n# Initialize list to store the dataset\ndataset = []\n\n# Open and read the CSV file\nwith open(csv_file_path, 'r') as csv_file:\n    reader = csv.DictReader(csv_file)\n    \n    for row in reader:\n        # Generate padded image ID to match filename\n        image_id = str(row[\"id\"]).zfill(6)\n        image_filename = f\"{image_id}.jpg\"\n        \n        # Construct the image path in your train folder\n        image_path = os.path.join(image_folder_path, image_filename)\n\n        # Ensure the image exists in the train folder before adding to the dataset\n        if os.path.exists(image_path):\n            # Construct human instruction with category information\n            human_instruction = prompt_template.replace(\"##CATEGORY##\", row[\"Category\"])\n            \n            # Construct the conversation structure as required\n            conversation = {\n                \"conversations\": [\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"<image>{human_instruction}\"\n                    },\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": json.dumps({f\"attr_{i}\": row.get(f\"attr_{i}\", \"\") for i in range(1, 11)})\n                    }\n                ],\n                \"images\": [image_path]\n            }\n\n            # Add the conversation to the dataset\n            dataset.append(conversation)\n        else:\n            print(f\"Image {image_filename} not found in {image_folder_path}, skipping.\")\n\n# Save the dataset to a JSON file\nwith open(output_json_file, 'w') as json_file:\n    json.dump(dataset, json_file, indent=4)\n\nprint(f\"Dataset successfully saved to {output_json_file}\")\n\n# Load existing dataset_info.json or create an empty dictionary if it doesn't exist\nif os.path.exists(dataset_info_file):\n    with open(dataset_info_file, 'r') as json_file:\n        dataset_info = json.load(json_file)\nelse:\n    dataset_info = {}\n\n# Add train_cat4 information to dataset_info\ndataset_info[\"cat5\"] = {\n    \"file_name\": \"cat5.json\",\n    \"formatting\": \"sharegpt\",\n    \"columns\": {\n        \"messages\": \"conversations\",\n        \"images\": \"images\"\n    },\n    \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n    }\n}\n\n# Save the updated dataset_info.json\nwith open(dataset_info_file, 'w') as json_file:\n    json.dump(dataset_info, json_file, indent=4)\n\nprint(f\"Dataset info successfully updated and saved to {dataset_info_file}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:30:45.889538Z","iopub.execute_input":"2024-11-16T07:30:45.889841Z","iopub.status.idle":"2024-11-16T07:30:46.864436Z","shell.execute_reply.started":"2024-11-16T07:30:45.889809Z","shell.execute_reply":"2024-11-16T07:30:46.863529Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/LLaMA-Factory\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Llama Board","metadata":{"id":"mXJwESjgf4Ex"}},{"cell_type":"code","source":"# import os\n\n# !GRADIO_SHARE=1 llamafactory-cli webui","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XPAfUgQPf1G4","outputId":"a7ce43c5-b767-4ef3-b449-4334dca059b1","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:30:46.865786Z","iopub.execute_input":"2024-11-16T07:30:46.866129Z","iopub.status.idle":"2024-11-16T07:30:46.870350Z","shell.execute_reply.started":"2024-11-16T07:30:46.866091Z","shell.execute_reply":"2024-11-16T07:30:46.869388Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Llama Factory CLI","metadata":{"id":"4MucaJW8gUg9"}},{"cell_type":"code","source":"import json\n\n\n\nargs = dict(\n\n  stage=\"sft\",                        # do supervised fine-tuning\n\n  do_train=True,\n\n  model_name_or_path=\"Qwen/Qwen2-VL-2B-Instruct\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n\n  dataset=\"cat1 , cat2 , cat3 , cat4 , cat5\",             # use alpaca and identity datasets\n\n  template=\"qwen2_vl\",                     # use llama3 prompt template\n\n  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n\n  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n\n  output_dir=\"qwen2vl_lora\",                  # the path to save LoRA adapters\n\n  per_device_train_batch_size=2,               # the batch size\n\n  gradient_accumulation_steps=4,               # the gradient accumulation steps\n\n  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n\n  logging_steps=10,                      # log every 10 steps\n\n  warmup_ratio=0.1,                      # use warmup scheduler\n\n  save_steps=500,                      # save checkpoint every 1000 steps\n\n  learning_rate=5e-5,                     # the learning rate\n\n  num_train_epochs=3.0,                    # the epochs of training\n\n  max_samples=5000,                      # use 500 examples in each dataset\n\n  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n\n  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n\n  fp16=True,                         # use float16 mixed precision training\n\n  use_liger_kernel=True,                   # use liger kernel for efficient training\n\n)","metadata":{"id":"tOw550N7gWaG","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:31:02.841358Z","iopub.execute_input":"2024-11-16T07:31:02.841657Z","iopub.status.idle":"2024-11-16T07:31:02.849352Z","shell.execute_reply.started":"2024-11-16T07:31:02.841621Z","shell.execute_reply":"2024-11-16T07:31:02.848429Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"json.dump(args, open(\"train_qwen2vl.json\", \"w\", encoding=\"utf-8\"), indent=2)","metadata":{"id":"tGQueoXYiM-F","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:31:02.853657Z","iopub.execute_input":"2024-11-16T07:31:02.853997Z","iopub.status.idle":"2024-11-16T07:31:02.864914Z","shell.execute_reply.started":"2024-11-16T07:31:02.853964Z","shell.execute_reply":"2024-11-16T07:31:02.864161Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:31:02.866111Z","iopub.execute_input":"2024-11-16T07:31:02.866475Z","iopub.status.idle":"2024-11-16T07:31:02.875019Z","shell.execute_reply.started":"2024-11-16T07:31:02.866434Z","shell.execute_reply":"2024-11-16T07:31:02.874162Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:31:02.876184Z","iopub.execute_input":"2024-11-16T07:31:02.876487Z","iopub.status.idle":"2024-11-16T07:31:02.883918Z","shell.execute_reply.started":"2024-11-16T07:31:02.876456Z","shell.execute_reply":"2024-11-16T07:31:02.883041Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"!llamafactory-cli train train_qwen2vl.json","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UoHMwFs8iX5Z","outputId":"f9663238-728b-410f-d5fc-9f8fb3e5112e","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:31:02.885181Z","iopub.execute_input":"2024-11-16T07:31:02.885951Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"[INFO|2024-11-16 07:31:32] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:24627\nW1116 07:31:34.098000 132972072752960 torch/distributed/run.py:779] \nW1116 07:31:34.098000 132972072752960 torch/distributed/run.py:779] *****************************************\nW1116 07:31:34.098000 132972072752960 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW1116 07:31:34.098000 132972072752960 torch/distributed/run.py:779] *****************************************\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n[WARNING|2024-11-16 07:31:44] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n[INFO|2024-11-16 07:31:44] llamafactory.hparams.parser:355 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n[INFO|2024-11-16 07:31:44] llamafactory.hparams.parser:355 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16\nconfig.json: 100%|█████████████████████████| 1.20k/1.20k [00:00<00:00, 8.83MB/s]\n[INFO|configuration_utils.py:679] 2024-11-16 07:31:44,622 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n[INFO|configuration_utils.py:746] 2024-11-16 07:31:44,625 >> Model config Qwen2VLConfig {\n  \"_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\",\n  \"architectures\": [\n    \"Qwen2VLForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1536,\n  \"image_token_id\": 151655,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8960,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 28,\n  \"model_type\": \"qwen2_vl\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": {\n    \"mrope_section\": [\n      16,\n      24,\n      24\n    ],\n    \"rope_type\": \"default\",\n    \"type\": \"default\"\n  },\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": 32768,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"video_token_id\": 151656,\n  \"vision_config\": {\n    \"hidden_size\": 1536,\n    \"in_chans\": 3,\n    \"model_type\": \"qwen2_vl\",\n    \"spatial_patch_size\": 14\n  },\n  \"vision_end_token_id\": 151653,\n  \"vision_start_token_id\": 151652,\n  \"vision_token_id\": 151654,\n  \"vocab_size\": 151936\n}\n\ntokenizer_config.json: 100%|███████████████| 4.19k/4.19k [00:00<00:00, 24.3MB/s]\nvocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 35.5MB/s]\nmerges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 18.9MB/s]\ntokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:00<00:00, 13.9MB/s]\n[INFO|tokenization_utils_base.py:2211] 2024-11-16 07:31:45,871 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/vocab.json\n[INFO|tokenization_utils_base.py:2211] 2024-11-16 07:31:45,871 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/merges.txt\n[INFO|tokenization_utils_base.py:2211] 2024-11-16 07:31:45,871 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer.json\n[INFO|tokenization_utils_base.py:2211] 2024-11-16 07:31:45,871 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2211] 2024-11-16 07:31:45,871 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2211] 2024-11-16 07:31:45,871 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2475] 2024-11-16 07:31:46,270 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\npreprocessor_config.json: 100%|████████████████| 347/347 [00:00<00:00, 2.71MB/s]\n[INFO|image_processing_base.py:375] 2024-11-16 07:31:46,484 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/preprocessor_config.json\n[INFO|image_processing_base.py:375] 2024-11-16 07:31:46,546 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/preprocessor_config.json\n[INFO|image_processing_base.py:429] 2024-11-16 07:31:46,547 >> Image processor Qwen2VLImageProcessor {\n  \"do_convert_rgb\": true,\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.48145466,\n    0.4578275,\n    0.40821073\n  ],\n  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n  \"image_std\": [\n    0.26862954,\n    0.26130258,\n    0.27577711\n  ],\n  \"max_pixels\": 12845056,\n  \"merge_size\": 2,\n  \"min_pixels\": 3136,\n  \"patch_size\": 14,\n  \"processor_class\": \"Qwen2VLProcessor\",\n  \"resample\": 3,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"max_pixels\": 12845056,\n    \"min_pixels\": 3136\n  },\n  \"temporal_patch_size\": 2\n}\n\n[INFO|tokenization_utils_base.py:2211] 2024-11-16 07:31:46,592 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/vocab.json\n[INFO|tokenization_utils_base.py:2211] 2024-11-16 07:31:46,592 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/merges.txt\n[INFO|tokenization_utils_base.py:2211] 2024-11-16 07:31:46,592 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer.json\n[INFO|tokenization_utils_base.py:2211] 2024-11-16 07:31:46,592 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2211] 2024-11-16 07:31:46,592 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2211] 2024-11-16 07:31:46,592 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2475] 2024-11-16 07:31:46,949 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nchat_template.json: 100%|██████████████████| 1.05k/1.05k [00:00<00:00, 6.01MB/s]\n[INFO|processing_utils.py:755] 2024-11-16 07:31:47,752 >> Processor Qwen2VLProcessor:\n- image_processor: Qwen2VLImageProcessor {\n  \"do_convert_rgb\": true,\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.48145466,\n    0.4578275,\n    0.40821073\n  ],\n  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n  \"image_std\": [\n    0.26862954,\n    0.26130258,\n    0.27577711\n  ],\n  \"max_pixels\": 12845056,\n  \"merge_size\": 2,\n  \"min_pixels\": 3136,\n  \"patch_size\": 14,\n  \"processor_class\": \"Qwen2VLProcessor\",\n  \"resample\": 3,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"max_pixels\": 12845056,\n    \"min_pixels\": 3136\n  },\n  \"temporal_patch_size\": 2\n}\n\n- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2-VL-2B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n\n{\n  \"processor_class\": \"Qwen2VLProcessor\"\n}\n\n[INFO|2024-11-16 07:31:47] llamafactory.data.template:157 >> Replace eos token: <|im_end|>\n[INFO|2024-11-16 07:31:47] llamafactory.data.loader:157 >> Loading dataset cat3.json...\nGenerating train split: 6822 examples [00:00, 46417.25 examples/s]\nConverting format of dataset: 100%|█| 6822/6822 [00:05<00:00, 1275.10 examples/s\nRunning tokenizer on dataset: 100%|██| 6822/6822 [02:50<00:00, 39.95 examples/s]\ntraining example:\ninput_ids:\n[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 22043, 279, 3897, 1985, 5582, 320, 42, 5639, 285, 8, 323, 1181, 12159, 2168, 11, 4486, 7023, 279, 2701, 8201, 369, 279, 1985, 25, 6376, 62, 16, 311, 6376, 62, 16, 15, 3118, 389, 8597, 13166, 12624, 323, 10295, 13, 29279, 429, 697, 19898, 525, 13382, 323, 9760, 311, 279, 9124, 17452, 304, 279, 2168, 13, 151645, 198, 151644, 77091, 198, 4913, 2991, 62, 16, 788, 330, 11453, 497, 330, 2991, 62, 17, 788, 330, 88192, 497, 330, 2991, 62, 18, 788, 330, 74, 33091, 3084, 497, 330, 2991, 62, 19, 788, 330, 48074, 497, 330, 2991, 62, 20, 788, 330, 4711, 497, 330, 2991, 62, 21, 788, 330, 31027, 497, 330, 2991, 62, 22, 788, 330, 31027, 497, 330, 2991, 62, 23, 788, 330, 27856, 57314, 42375, 497, 330, 2991, 62, 24, 788, 330, 22308, 497, 330, 2991, 62, 16, 15, 788, 330, 2152, 9207, 151645]\ninputs:\n<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>Given the provided product category (Kurtis) and its corresponding image, please predict the following attributes for the product: attr_1 to attr_10 based on previously observed patterns and examples. Ensure that your predictions are accurate and relevant to the visual characteristics in the image.<|im_end|>\n<|im_start|>assistant\n{\"attr_1\": \"black\", \"attr_2\": \"straight\", \"attr_3\": \"knee length\", \"attr_4\": \"daily\", \"attr_5\": \"net\", \"attr_6\": \"solid\", \"attr_7\": \"solid\", \"attr_8\": \"three-quarter sleeves\", \"attr_9\": \"regular\", \"attr_10\": \"no\"}<|im_end|>\nlabel_ids:\n[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4913, 2991, 62, 16, 788, 330, 11453, 497, 330, 2991, 62, 17, 788, 330, 88192, 497, 330, 2991, 62, 18, 788, 330, 74, 33091, 3084, 497, 330, 2991, 62, 19, 788, 330, 48074, 497, 330, 2991, 62, 20, 788, 330, 4711, 497, 330, 2991, 62, 21, 788, 330, 31027, 497, 330, 2991, 62, 22, 788, 330, 31027, 497, 330, 2991, 62, 23, 788, 330, 27856, 57314, 42375, 497, 330, 2991, 62, 24, 788, 330, 22308, 497, 330, 2991, 62, 16, 15, 788, 330, 2152, 9207, 151645]\nlabels:\n{\"attr_1\": \"black\", \"attr_2\": \"straight\", \"attr_3\": \"knee length\", \"attr_4\": \"daily\", \"attr_5\": \"net\", \"attr_6\": \"solid\", \"attr_7\": \"solid\", \"attr_8\": \"three-quarter sleeves\", \"attr_9\": \"regular\", \"attr_10\": \"no\"}<|im_end|>\n[INFO|configuration_utils.py:679] 2024-11-16 07:34:44,857 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n[INFO|configuration_utils.py:746] 2024-11-16 07:34:44,860 >> Model config Qwen2VLConfig {\n  \"_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\",\n  \"architectures\": [\n    \"Qwen2VLForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1536,\n  \"image_token_id\": 151655,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8960,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 28,\n  \"model_type\": \"qwen2_vl\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": {\n    \"mrope_section\": [\n      16,\n      24,\n      24\n    ],\n    \"rope_type\": \"default\",\n    \"type\": \"default\"\n  },\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": 32768,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"video_token_id\": 151656,\n  \"vision_config\": {\n    \"hidden_size\": 1536,\n    \"in_chans\": 3,\n    \"model_type\": \"qwen2_vl\",\n    \"spatial_patch_size\": 14\n  },\n  \"vision_end_token_id\": 151653,\n  \"vision_start_token_id\": 151652,\n  \"vision_token_id\": 151654,\n  \"vocab_size\": 151936\n}\n\nmodel.safetensors.index.json: 100%|█████████| 56.4k/56.4k [00:00<00:00, 131MB/s]\n[INFO|modeling_utils.py:3937] 2024-11-16 07:34:45,080 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/model.safetensors.index.json\nDownloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\nmodel-00001-of-00002.safetensors:   0%|             | 0.00/3.99G [00:00<?, ?B/s]\u001b[A\nmodel-00001-of-00002.safetensors:   0%|    | 10.5M/3.99G [00:00<01:39, 39.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   1%|    | 21.0M/3.99G [00:00<01:35, 41.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   1%|    | 31.5M/3.99G [00:00<01:34, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   1%|    | 41.9M/3.99G [00:01<01:33, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   1%|    | 52.4M/3.99G [00:01<01:32, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   2%|    | 62.9M/3.99G [00:01<01:32, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   2%|    | 73.4M/3.99G [00:01<01:32, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   2%|    | 83.9M/3.99G [00:01<01:32, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   2%|    | 94.4M/3.99G [00:02<01:31, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   3%|▏    | 105M/3.99G [00:02<01:31, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   3%|▏    | 115M/3.99G [00:02<01:31, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   3%|▏    | 126M/3.99G [00:02<01:30, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   3%|▏    | 136M/3.99G [00:03<01:30, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   4%|▏    | 147M/3.99G [00:03<01:29, 42.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   4%|▏    | 157M/3.99G [00:03<01:29, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   4%|▏    | 168M/3.99G [00:03<01:29, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   4%|▏    | 178M/3.99G [00:04<01:29, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   5%|▏    | 189M/3.99G [00:04<01:29, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   5%|▏    | 199M/3.99G [00:04<01:29, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   5%|▎    | 210M/3.99G [00:04<01:29, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   6%|▎    | 220M/3.99G [00:05<01:28, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   6%|▎    | 231M/3.99G [00:05<01:28, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   6%|▎    | 241M/3.99G [00:05<01:28, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   6%|▎    | 252M/3.99G [00:05<01:28, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   7%|▎    | 262M/3.99G [00:06<01:27, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   7%|▎    | 273M/3.99G [00:06<01:27, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   7%|▎    | 283M/3.99G [00:06<01:27, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   7%|▎    | 294M/3.99G [00:06<01:26, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   8%|▍    | 304M/3.99G [00:07<01:26, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   8%|▍    | 315M/3.99G [00:07<01:26, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   8%|▍    | 325M/3.99G [00:07<01:25, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   8%|▍    | 336M/3.99G [00:07<01:26, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   9%|▍    | 346M/3.99G [00:08<01:26, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   9%|▍    | 357M/3.99G [00:08<01:25, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   9%|▍    | 367M/3.99G [00:08<01:25, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   9%|▍    | 377M/3.99G [00:08<01:24, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  10%|▍    | 388M/3.99G [00:09<01:24, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  10%|▍    | 398M/3.99G [00:09<01:24, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  10%|▌    | 409M/3.99G [00:09<01:23, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  11%|▌    | 419M/3.99G [00:09<01:23, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  11%|▌    | 430M/3.99G [00:10<01:23, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  11%|▌    | 440M/3.99G [00:10<01:23, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  11%|▌    | 451M/3.99G [00:10<01:23, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  12%|▌    | 461M/3.99G [00:10<01:22, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  12%|▌    | 472M/3.99G [00:11<01:22, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  12%|▌    | 482M/3.99G [00:11<01:22, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  12%|▌    | 493M/3.99G [00:11<01:22, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  13%|▋    | 503M/3.99G [00:11<01:21, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  13%|▋    | 514M/3.99G [00:12<01:21, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  13%|▋    | 524M/3.99G [00:12<01:21, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  13%|▋    | 535M/3.99G [00:12<01:21, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  14%|▋    | 545M/3.99G [00:12<01:21, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  14%|▋    | 556M/3.99G [00:13<01:21, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  14%|▋    | 566M/3.99G [00:13<01:21, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  14%|▋    | 577M/3.99G [00:13<01:20, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  15%|▋    | 587M/3.99G [00:13<01:19, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  15%|▋    | 598M/3.99G [00:14<01:20, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  15%|▊    | 608M/3.99G [00:14<01:19, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  16%|▊    | 619M/3.99G [00:14<01:19, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  16%|▊    | 629M/3.99G [00:14<01:19, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  16%|▊    | 640M/3.99G [00:15<01:18, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  16%|▊    | 650M/3.99G [00:15<01:18, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  17%|▊    | 661M/3.99G [00:15<01:18, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  17%|▊    | 671M/3.99G [00:15<01:18, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  17%|▊    | 682M/3.99G [00:16<01:18, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  17%|▊    | 692M/3.99G [00:16<01:17, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  18%|▉    | 703M/3.99G [00:16<01:17, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  18%|▉    | 713M/3.99G [00:16<01:17, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  18%|▉    | 724M/3.99G [00:17<01:16, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  18%|▉    | 734M/3.99G [00:17<01:16, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  19%|▉    | 744M/3.99G [00:17<01:31, 35.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  19%|▉    | 755M/3.99G [00:17<01:26, 37.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  19%|▉    | 765M/3.99G [00:18<01:23, 38.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  19%|▉    | 776M/3.99G [00:18<01:20, 39.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  20%|▉    | 786M/3.99G [00:18<01:18, 40.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  20%|▉    | 797M/3.99G [00:18<01:17, 41.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  20%|█    | 807M/3.99G [00:19<01:16, 41.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  21%|█    | 818M/3.99G [00:19<01:15, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  21%|█    | 828M/3.99G [00:19<01:15, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  21%|█    | 839M/3.99G [00:19<01:15, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  21%|█    | 849M/3.99G [00:20<01:14, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  22%|█    | 860M/3.99G [00:20<01:14, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  22%|█    | 870M/3.99G [00:20<01:14, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  22%|█    | 881M/3.99G [00:20<01:14, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  22%|█    | 891M/3.99G [00:21<01:13, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  23%|█▏   | 902M/3.99G [00:21<01:13, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  23%|█▏   | 912M/3.99G [00:21<01:12, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  23%|█▏   | 923M/3.99G [00:21<01:12, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  23%|█▏   | 933M/3.99G [00:22<01:12, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  24%|█▏   | 944M/3.99G [00:22<01:11, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  24%|█▏   | 954M/3.99G [00:22<01:11, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  24%|█▏   | 965M/3.99G [00:22<01:11, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  24%|█▏   | 975M/3.99G [00:23<01:11, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  25%|█▏   | 986M/3.99G [00:23<01:11, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  25%|█▏   | 996M/3.99G [00:23<01:10, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  25%|█   | 1.01G/3.99G [00:23<01:09, 42.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  26%|█   | 1.02G/3.99G [00:24<01:09, 42.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  26%|█   | 1.03G/3.99G [00:24<01:09, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  26%|█   | 1.04G/3.99G [00:24<01:08, 42.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  26%|█   | 1.05G/3.99G [00:24<01:08, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  27%|█   | 1.06G/3.99G [00:25<01:08, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  27%|█   | 1.07G/3.99G [00:25<01:08, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  27%|█   | 1.08G/3.99G [00:25<01:08, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  27%|█   | 1.09G/3.99G [00:25<01:07, 42.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  28%|█   | 1.10G/3.99G [00:26<01:07, 42.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  28%|█   | 1.11G/3.99G [00:26<01:07, 42.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  28%|█▏  | 1.12G/3.99G [00:26<01:06, 42.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  28%|█▏  | 1.13G/3.99G [00:26<01:06, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  29%|█▏  | 1.14G/3.99G [00:27<01:06, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  29%|█▏  | 1.15G/3.99G [00:27<01:06, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  29%|█▏  | 1.16G/3.99G [00:27<01:06, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  29%|█▏  | 1.17G/3.99G [00:27<01:06, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  30%|█▏  | 1.18G/3.99G [00:28<01:05, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  30%|█▏  | 1.20G/3.99G [00:28<01:05, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  30%|█▏  | 1.21G/3.99G [00:28<01:05, 42.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  30%|█▏  | 1.22G/3.99G [00:28<01:05, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  31%|█▏  | 1.23G/3.99G [00:29<01:04, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  31%|█▏  | 1.24G/3.99G [00:29<01:05, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  31%|█▎  | 1.25G/3.99G [00:29<01:04, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  32%|█▎  | 1.26G/3.99G [00:29<01:04, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  32%|█▎  | 1.27G/3.99G [00:30<01:16, 35.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  32%|█▎  | 1.29G/3.99G [00:30<01:00, 44.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  33%|█▎  | 1.30G/3.99G [00:30<01:02, 43.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  33%|█▎  | 1.31G/3.99G [00:31<01:01, 43.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  33%|█▎  | 1.32G/3.99G [00:31<01:01, 43.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  33%|█▎  | 1.33G/3.99G [00:31<01:01, 43.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  34%|█▎  | 1.34G/3.99G [00:31<01:03, 41.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  34%|█▎  | 1.35G/3.99G [00:32<01:02, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  34%|█▎  | 1.36G/3.99G [00:32<01:01, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  34%|█▍  | 1.37G/3.99G [00:32<01:01, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  35%|█▍  | 1.38G/3.99G [00:32<01:02, 41.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  35%|█▍  | 1.39G/3.99G [00:33<01:01, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  35%|█▍  | 1.41G/3.99G [00:33<01:01, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  35%|█▍  | 1.42G/3.99G [00:33<01:00, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  36%|█▍  | 1.43G/3.99G [00:33<01:01, 41.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  36%|█▍  | 1.44G/3.99G [00:34<01:00, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  36%|█▍  | 1.45G/3.99G [00:34<01:00, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  37%|█▍  | 1.46G/3.99G [00:34<00:59, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  37%|█▍  | 1.47G/3.99G [00:34<01:00, 41.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  37%|█▍  | 1.48G/3.99G [00:35<00:59, 41.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  37%|█▍  | 1.49G/3.99G [00:35<00:59, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  38%|█▌  | 1.50G/3.99G [00:35<00:59, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  38%|█▌  | 1.51G/3.99G [00:35<00:59, 41.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  38%|█▌  | 1.52G/3.99G [00:36<00:58, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  38%|█▌  | 1.53G/3.99G [00:36<00:58, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  39%|█▌  | 1.54G/3.99G [00:36<00:57, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  39%|█▌  | 1.55G/3.99G [00:36<00:58, 41.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  39%|█▌  | 1.56G/3.99G [00:37<00:57, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  39%|█▌  | 1.57G/3.99G [00:37<00:57, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  40%|█▌  | 1.58G/3.99G [00:37<00:56, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  40%|█▌  | 1.59G/3.99G [00:37<00:57, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  40%|█▌  | 1.60G/3.99G [00:38<00:56, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  40%|█▌  | 1.61G/3.99G [00:38<00:56, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  41%|█▋  | 1.63G/3.99G [00:38<00:55, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  41%|█▋  | 1.64G/3.99G [00:38<00:56, 41.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  41%|█▋  | 1.65G/3.99G [00:39<00:56, 41.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|█▋  | 1.66G/3.99G [00:39<00:55, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|█▋  | 1.67G/3.99G [00:39<00:54, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|█▋  | 1.68G/3.99G [00:39<00:55, 41.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|█▋  | 1.69G/3.99G [00:40<00:54, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  43%|█▋  | 1.70G/3.99G [00:40<01:04, 35.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  43%|█▋  | 1.72G/3.99G [00:40<00:52, 43.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  43%|█▋  | 1.73G/3.99G [00:41<00:52, 43.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  44%|█▋  | 1.74G/3.99G [00:41<00:52, 43.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  44%|█▊  | 1.75G/3.99G [00:41<00:52, 42.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  44%|█▊  | 1.76G/3.99G [00:41<00:53, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  44%|█▊  | 1.77G/3.99G [00:42<00:52, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  45%|█▊  | 1.78G/3.99G [00:42<00:51, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  45%|█▊  | 1.79G/3.99G [00:42<00:51, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  45%|█▊  | 1.80G/3.99G [00:42<00:52, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  45%|█▊  | 1.81G/3.99G [00:43<00:51, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  46%|█▊  | 1.82G/3.99G [00:43<00:51, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  46%|█▊  | 1.84G/3.99G [00:43<00:50, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  46%|█▊  | 1.85G/3.99G [00:43<00:51, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  47%|█▊  | 1.86G/3.99G [00:44<00:50, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  47%|█▊  | 1.87G/3.99G [00:44<00:50, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  47%|█▉  | 1.88G/3.99G [00:44<00:50, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  47%|█▉  | 1.89G/3.99G [00:44<00:50, 41.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  48%|█▉  | 1.90G/3.99G [00:45<00:49, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  48%|█▉  | 1.91G/3.99G [00:45<00:49, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  48%|█▉  | 1.92G/3.99G [00:45<00:49, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  48%|█▉  | 1.93G/3.99G [00:45<00:49, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  49%|█▉  | 1.94G/3.99G [00:46<00:48, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  49%|█▉  | 1.95G/3.99G [00:46<00:50, 40.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  49%|█▉  | 1.96G/3.99G [00:46<00:49, 40.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  49%|█▉  | 1.97G/3.99G [00:46<00:48, 41.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  50%|█▉  | 1.98G/3.99G [00:47<00:47, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  50%|█▉  | 1.99G/3.99G [00:47<00:47, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  50%|██  | 2.00G/3.99G [00:47<00:47, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  50%|██  | 2.01G/3.99G [00:47<00:46, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  51%|██  | 2.02G/3.99G [00:48<00:46, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  51%|██  | 2.03G/3.99G [00:48<00:45, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  51%|██  | 2.04G/3.99G [00:48<00:45, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  52%|██  | 2.06G/3.99G [00:48<00:46, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  52%|██  | 2.07G/3.99G [00:49<00:45, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  52%|██  | 2.08G/3.99G [00:49<00:45, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  52%|██  | 2.09G/3.99G [00:49<00:45, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  53%|██  | 2.10G/3.99G [00:49<00:45, 41.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  53%|██  | 2.11G/3.99G [00:50<00:44, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  53%|██  | 2.12G/3.99G [00:50<00:44, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  53%|██▏ | 2.13G/3.99G [00:50<00:43, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  54%|██▏ | 2.14G/3.99G [00:50<00:44, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  54%|██▏ | 2.15G/3.99G [00:51<00:43, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  54%|██▏ | 2.16G/3.99G [00:51<00:43, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  54%|██▏ | 2.17G/3.99G [00:51<00:42, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  55%|██▏ | 2.18G/3.99G [00:51<00:43, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  55%|██▏ | 2.19G/3.99G [00:52<00:42, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  55%|██▏ | 2.20G/3.99G [00:52<00:42, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  55%|██▏ | 2.21G/3.99G [00:52<00:42, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  56%|██▏ | 2.22G/3.99G [00:52<00:42, 41.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  56%|██▏ | 2.23G/3.99G [00:53<00:41, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  56%|██▎ | 2.24G/3.99G [00:53<00:52, 33.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  57%|██▎ | 2.26G/3.99G [00:53<00:38, 44.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  57%|██▎ | 2.28G/3.99G [00:54<00:38, 44.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  57%|██▎ | 2.29G/3.99G [00:54<00:38, 43.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  58%|██▎ | 2.30G/3.99G [00:54<00:38, 43.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  58%|██▎ | 2.31G/3.99G [00:54<00:39, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  58%|██▎ | 2.32G/3.99G [00:55<00:39, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  58%|██▎ | 2.33G/3.99G [00:55<00:39, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  59%|██▎ | 2.34G/3.99G [00:55<00:39, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  59%|██▎ | 2.35G/3.99G [00:55<00:39, 41.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  59%|██▎ | 2.36G/3.99G [00:56<00:38, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  59%|██▍ | 2.37G/3.99G [00:56<00:38, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  60%|██▍ | 2.38G/3.99G [00:56<00:37, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  60%|██▍ | 2.39G/3.99G [00:56<00:38, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  60%|██▍ | 2.40G/3.99G [00:57<00:37, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  60%|██▍ | 2.41G/3.99G [00:57<00:37, 41.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  61%|██▍ | 2.42G/3.99G [00:57<00:37, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  61%|██▍ | 2.43G/3.99G [00:57<00:37, 41.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  61%|██▍ | 2.44G/3.99G [00:58<00:36, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  62%|██▍ | 2.45G/3.99G [00:58<00:36, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  62%|██▍ | 2.46G/3.99G [00:58<00:35, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  62%|██▍ | 2.47G/3.99G [00:58<00:36, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  62%|██▍ | 2.49G/3.99G [00:59<00:35, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  63%|██▌ | 2.50G/3.99G [00:59<00:35, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  63%|██▌ | 2.51G/3.99G [00:59<00:35, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  63%|██▌ | 2.52G/3.99G [00:59<00:35, 41.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  63%|██▌ | 2.53G/3.99G [01:00<00:35, 41.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  64%|██▌ | 2.54G/3.99G [01:00<00:34, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  64%|██▌ | 2.55G/3.99G [01:00<00:34, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  64%|██▌ | 2.56G/3.99G [01:00<00:34, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  64%|██▌ | 2.57G/3.99G [01:01<00:33, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  65%|██▌ | 2.58G/3.99G [01:01<00:33, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  65%|██▌ | 2.59G/3.99G [01:01<00:33, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  65%|██▌ | 2.60G/3.99G [01:01<00:33, 41.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  65%|██▌ | 2.61G/3.99G [01:02<00:32, 41.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  66%|██▋ | 2.62G/3.99G [01:02<00:32, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  66%|██▋ | 2.63G/3.99G [01:02<00:32, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  66%|██▋ | 2.64G/3.99G [01:02<00:32, 41.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  67%|██▋ | 2.65G/3.99G [01:03<00:31, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  67%|██▋ | 2.66G/3.99G [01:03<00:31, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  67%|██▋ | 2.67G/3.99G [01:03<00:31, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  67%|██▋ | 2.68G/3.99G [01:03<00:31, 41.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  68%|██▋ | 2.69G/3.99G [01:04<00:30, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  68%|██▋ | 2.71G/3.99G [01:04<00:30, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  68%|██▋ | 2.72G/3.99G [01:04<00:30, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  68%|██▋ | 2.73G/3.99G [01:04<00:30, 41.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  69%|██▋ | 2.74G/3.99G [01:05<00:29, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  69%|██▊ | 2.75G/3.99G [01:05<00:29, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  69%|██▊ | 2.76G/3.99G [01:05<00:29, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  69%|██▊ | 2.77G/3.99G [01:05<00:29, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  70%|██▊ | 2.78G/3.99G [01:06<00:28, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  70%|██▊ | 2.79G/3.99G [01:06<00:28, 41.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  70%|██▊ | 2.80G/3.99G [01:06<00:29, 39.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  70%|██▊ | 2.81G/3.99G [01:06<00:27, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  71%|██▊ | 2.82G/3.99G [01:07<00:27, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  71%|██▊ | 2.83G/3.99G [01:07<00:27, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  71%|██▊ | 2.84G/3.99G [01:07<00:26, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  72%|██▊ | 2.85G/3.99G [01:07<00:27, 41.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  72%|██▊ | 2.86G/3.99G [01:08<00:26, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  72%|██▉ | 2.87G/3.99G [01:08<00:26, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  72%|██▉ | 2.88G/3.99G [01:08<00:26, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  73%|██▉ | 2.89G/3.99G [01:08<00:26, 41.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  73%|██▉ | 2.90G/3.99G [01:09<00:25, 41.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  73%|██▉ | 2.92G/3.99G [01:09<00:25, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  73%|██▉ | 2.93G/3.99G [01:09<00:25, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  74%|██▉ | 2.94G/3.99G [01:09<00:25, 41.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  74%|██▉ | 2.95G/3.99G [01:10<00:24, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  74%|██▉ | 2.96G/3.99G [01:10<00:24, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  74%|██▉ | 2.97G/3.99G [01:10<00:24, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  75%|██▉ | 2.98G/3.99G [01:10<00:24, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  75%|██▉ | 2.99G/3.99G [01:11<00:23, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  75%|███ | 3.00G/3.99G [01:11<00:23, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  75%|███ | 3.01G/3.99G [01:11<00:23, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  76%|███ | 3.02G/3.99G [01:11<00:23, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  76%|███ | 3.03G/3.99G [01:12<00:22, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  76%|███ | 3.04G/3.99G [01:12<00:22, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  77%|███ | 3.05G/3.99G [01:12<00:22, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  77%|███ | 3.06G/3.99G [01:12<00:22, 41.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  77%|███ | 3.07G/3.99G [01:13<00:21, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  77%|███ | 3.08G/3.99G [01:13<00:21, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  78%|███ | 3.09G/3.99G [01:13<00:21, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  78%|███ | 3.10G/3.99G [01:13<00:21, 41.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  78%|███ | 3.11G/3.99G [01:14<00:20, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  78%|███▏| 3.12G/3.99G [01:14<00:20, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  79%|███▏| 3.14G/3.99G [01:14<00:20, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  79%|███▏| 3.15G/3.99G [01:14<00:20, 41.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  79%|███▏| 3.16G/3.99G [01:15<00:19, 41.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  79%|███▏| 3.17G/3.99G [01:15<00:19, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  80%|███▏| 3.18G/3.99G [01:15<00:19, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  80%|███▏| 3.19G/3.99G [01:15<00:19, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  80%|███▏| 3.20G/3.99G [01:16<00:18, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  80%|███▏| 3.21G/3.99G [01:16<00:18, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  81%|███▏| 3.22G/3.99G [01:16<00:18, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  81%|███▏| 3.23G/3.99G [01:16<00:18, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  81%|███▏| 3.24G/3.99G [01:17<00:17, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  81%|███▎| 3.25G/3.99G [01:17<00:17, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  82%|███▎| 3.26G/3.99G [01:17<00:17, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  82%|███▎| 3.27G/3.99G [01:17<00:18, 38.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  82%|███▎| 3.28G/3.99G [01:18<00:16, 42.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  83%|███▎| 3.29G/3.99G [01:18<00:16, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  83%|███▎| 3.30G/3.99G [01:18<00:16, 42.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  83%|███▎| 3.31G/3.99G [01:18<00:16, 41.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  83%|███▎| 3.32G/3.99G [01:19<00:15, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  84%|███▎| 3.33G/3.99G [01:19<00:15, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  84%|███▎| 3.34G/3.99G [01:19<00:15, 41.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  84%|███▎| 3.36G/3.99G [01:19<00:15, 41.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  84%|███▍| 3.37G/3.99G [01:20<00:14, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  85%|███▍| 3.38G/3.99G [01:20<00:14, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  85%|███▍| 3.39G/3.99G [01:20<00:14, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  85%|███▍| 3.40G/3.99G [01:20<00:14, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  85%|███▍| 3.41G/3.99G [01:21<00:13, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  86%|███▍| 3.42G/3.99G [01:21<00:13, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  86%|███▍| 3.43G/3.99G [01:21<00:13, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  86%|███▍| 3.44G/3.99G [01:21<00:13, 41.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  86%|███▍| 3.45G/3.99G [01:22<00:12, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  87%|███▍| 3.46G/3.99G [01:22<00:12, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  87%|███▍| 3.47G/3.99G [01:22<00:12, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  87%|███▍| 3.48G/3.99G [01:22<00:12, 41.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  88%|███▌| 3.49G/3.99G [01:23<00:11, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  88%|███▌| 3.50G/3.99G [01:23<00:11, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  88%|███▌| 3.51G/3.99G [01:23<00:11, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  88%|███▌| 3.52G/3.99G [01:23<00:11, 41.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  89%|███▌| 3.53G/3.99G [01:24<00:10, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  89%|███▌| 3.54G/3.99G [01:24<00:10, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  89%|███▌| 3.55G/3.99G [01:24<00:10, 42.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  89%|███▌| 3.57G/3.99G [01:24<00:10, 41.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  90%|███▌| 3.58G/3.99G [01:25<00:09, 41.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  90%|███▌| 3.59G/3.99G [01:25<00:09, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  90%|███▌| 3.60G/3.99G [01:25<00:09, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  90%|███▌| 3.61G/3.99G [01:25<00:09, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  91%|███▋| 3.62G/3.99G [01:26<00:08, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  91%|███▋| 3.63G/3.99G [01:26<00:08, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  91%|███▋| 3.64G/3.99G [01:26<00:08, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  91%|███▋| 3.65G/3.99G [01:26<00:08, 41.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  92%|███▋| 3.66G/3.99G [01:27<00:07, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  92%|███▋| 3.67G/3.99G [01:27<00:07, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  92%|███▋| 3.68G/3.99G [01:27<00:07, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  93%|███▋| 3.69G/3.99G [01:27<00:07, 41.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  93%|███▋| 3.70G/3.99G [01:28<00:06, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  93%|███▋| 3.71G/3.99G [01:28<00:06, 39.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  93%|███▋| 3.72G/3.99G [01:28<00:06, 43.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  94%|███▋| 3.73G/3.99G [01:28<00:06, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  94%|███▊| 3.74G/3.99G [01:29<00:05, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  94%|███▊| 3.75G/3.99G [01:29<00:05, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  94%|███▊| 3.76G/3.99G [01:29<00:05, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  95%|███▊| 3.77G/3.99G [01:29<00:05, 41.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  95%|███▊| 3.79G/3.99G [01:30<00:04, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  95%|███▊| 3.80G/3.99G [01:30<00:04, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  95%|███▊| 3.81G/3.99G [01:30<00:04, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  96%|███▊| 3.82G/3.99G [01:30<00:04, 41.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  96%|███▊| 3.83G/3.99G [01:31<00:03, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  96%|███▊| 3.84G/3.99G [01:31<00:03, 42.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  96%|███▊| 3.85G/3.99G [01:31<00:03, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  97%|███▊| 3.86G/3.99G [01:31<00:03, 41.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  97%|███▉| 3.87G/3.99G [01:32<00:02, 41.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  97%|███▉| 3.88G/3.99G [01:32<00:02, 41.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  98%|███▉| 3.89G/3.99G [01:32<00:02, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  98%|███▉| 3.90G/3.99G [01:32<00:02, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  98%|███▉| 3.91G/3.99G [01:33<00:01, 42.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  98%|███▉| 3.92G/3.99G [01:33<00:01, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  99%|███▉| 3.93G/3.99G [01:33<00:01, 42.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  99%|███▉| 3.94G/3.99G [01:33<00:01, 41.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  99%|███▉| 3.95G/3.99G [01:34<00:00, 41.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  99%|███▉| 3.96G/3.99G [01:34<00:00, 42.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors: 100%|███▉| 3.97G/3.99G [01:34<00:00, 42.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors: 100%|████| 3.99G/3.99G [01:34<00:00, 42.0MB/s]\u001b[A\nDownloading shards:  50%|████████████▌            | 1/2 [01:35<01:35, 95.03s/it]\nmodel-00002-of-00002.safetensors:   0%|              | 0.00/429M [00:00<?, ?B/s]\u001b[A\nmodel-00002-of-00002.safetensors:   2%|     | 10.5M/429M [00:00<00:09, 43.2MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   5%|▏    | 21.0M/429M [00:00<00:09, 42.7MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   7%|▎    | 31.5M/429M [00:00<00:09, 42.4MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  10%|▍    | 41.9M/429M [00:00<00:09, 42.5MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  12%|▌    | 52.4M/429M [00:01<00:08, 42.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  15%|▋    | 62.9M/429M [00:01<00:08, 42.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  17%|▊    | 73.4M/429M [00:01<00:08, 42.9MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  20%|▉    | 83.9M/429M [00:01<00:08, 42.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  22%|█    | 94.4M/429M [00:02<00:07, 42.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  24%|█▍    | 105M/429M [00:02<00:07, 42.7MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  27%|█▌    | 115M/429M [00:02<00:07, 42.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  29%|█▊    | 126M/429M [00:02<00:07, 42.7MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  32%|█▉    | 136M/429M [00:03<00:06, 42.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  34%|██    | 147M/429M [00:03<00:06, 42.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  37%|██▏   | 157M/429M [00:03<00:06, 42.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  39%|██▎   | 168M/429M [00:03<00:06, 42.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  42%|██▍   | 178M/429M [00:04<00:06, 41.0MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  44%|██▋   | 189M/429M [00:04<00:06, 38.1MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  46%|██▊   | 199M/429M [00:05<00:07, 30.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  49%|██▉   | 210M/429M [00:05<00:06, 35.5MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  51%|███   | 220M/429M [00:05<00:05, 35.7MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  54%|███▏  | 231M/429M [00:05<00:05, 35.5MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  56%|███▎  | 241M/429M [00:06<00:05, 33.4MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  59%|███▌  | 252M/429M [00:06<00:05, 33.3MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  61%|███▋  | 262M/429M [00:06<00:04, 33.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  63%|███▊  | 273M/429M [00:07<00:04, 33.0MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  66%|███▉  | 283M/429M [00:07<00:04, 32.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  68%|████  | 294M/429M [00:07<00:04, 31.3MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  71%|████▏ | 304M/429M [00:08<00:03, 32.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  73%|████▍ | 315M/429M [00:08<00:03, 31.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  76%|████▌ | 325M/429M [00:08<00:03, 31.2MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  78%|████▋ | 336M/429M [00:09<00:03, 30.6MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  81%|████▊ | 346M/429M [00:09<00:02, 33.1MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  83%|████▉ | 357M/429M [00:09<00:02, 35.5MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  85%|█████▏| 367M/429M [00:09<00:01, 37.2MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  88%|█████▎| 377M/429M [00:10<00:01, 38.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  90%|█████▍| 388M/429M [00:10<00:01, 39.8MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  93%|█████▌| 398M/429M [00:10<00:00, 40.7MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  95%|█████▋| 409M/429M [00:10<00:00, 41.5MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  98%|█████▊| 419M/429M [00:11<00:00, 41.7MB/s]\u001b[A\nmodel-00002-of-00002.safetensors: 100%|██████| 429M/429M [00:11<00:00, 37.8MB/s]\u001b[A\nDownloading shards: 100%|█████████████████████████| 2/2 [01:46<00:00, 53.24s/it]\n[INFO|modeling_utils.py:1670] 2024-11-16 07:36:31,562 >> Instantiating Qwen2VLForConditionalGeneration model under default dtype torch.float16.\n[INFO|configuration_utils.py:1096] 2024-11-16 07:36:31,564 >> Generate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645\n}\n\n[INFO|modeling_utils.py:1670] 2024-11-16 07:36:31,565 >> Instantiating Qwen2VisionTransformerPretrainedModel model under default dtype torch.float16.\nDownloading shards: 100%|█████████████████████████| 2/2 [01:46<00:00, 53.25s/it]\n[WARNING|logging.py:168] 2024-11-16 07:36:31,634 >> `Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.83s/it]\ngeneration_config.json: 100%|██████████████████| 272/272 [00:00<00:00, 1.44MB/s]\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.96s/it]\n[INFO|modeling_utils.py:4800] 2024-11-16 07:36:37,757 >> All model checkpoint weights were used when initializing Qwen2VLForConditionalGeneration.\n\n[INFO|modeling_utils.py:4808] 2024-11-16 07:36:37,757 >> All the weights of Qwen2VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2VLForConditionalGeneration for predictions without further training.\n[INFO|configuration_utils.py:1051] 2024-11-16 07:36:37,811 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/generation_config.json\n[INFO|configuration_utils.py:1096] 2024-11-16 07:36:37,811 >> Generate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    151645,\n    151643\n  ],\n  \"pad_token_id\": 151643,\n  \"temperature\": 0.01,\n  \"top_k\": 1,\n  \"top_p\": 0.001\n}\n\n[INFO|2024-11-16 07:36:37] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n[INFO|2024-11-16 07:36:37] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n[INFO|2024-11-16 07:36:37] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n[INFO|2024-11-16 07:36:37] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n[INFO|2024-11-16 07:36:37] llamafactory.model.model_utils.misc:157 >> Found linear modules: up_proj,gate_proj,o_proj,q_proj,v_proj,down_proj,k_proj\n/kaggle/working/LLaMA-Factory/src/llamafactory/train/sft/trainer.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(**kwargs)\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nThe model is not an instance of PreTrainedModel. No liger kernels will be applied.\n[INFO|2024-11-16 07:36:38] llamafactory.model.loader:157 >> trainable params: 9,232,384 || all params: 2,218,217,984 || trainable%: 0.4162\n/kaggle/working/LLaMA-Factory/src/llamafactory/train/sft/trainer.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(**kwargs)\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n[WARNING|trainer.py:497] 2024-11-16 07:36:38,575 >> The model is not an instance of PreTrainedModel. No liger kernels will be applied.\n[INFO|trainer.py:698] 2024-11-16 07:36:38,634 >> Using auto half precision backend\n[INFO|2024-11-16 07:36:38] llamafactory.train.trainer_utils:157 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.\n[INFO|trainer.py:2313] 2024-11-16 07:36:39,227 >> ***** Running training *****\n[INFO|trainer.py:2314] 2024-11-16 07:36:39,227 >>   Num examples = 6,822\n[INFO|trainer.py:2315] 2024-11-16 07:36:39,227 >>   Num Epochs = 3\n[INFO|trainer.py:2316] 2024-11-16 07:36:39,227 >>   Instantaneous batch size per device = 2\n[INFO|trainer.py:2319] 2024-11-16 07:36:39,227 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n[INFO|trainer.py:2320] 2024-11-16 07:36:39,227 >>   Gradient Accumulation steps = 4\n[INFO|trainer.py:2321] 2024-11-16 07:36:39,227 >>   Total optimization steps = 1,278\n[INFO|trainer.py:2322] 2024-11-16 07:36:39,237 >>   Number of trainable parameters = 9,232,384\n  0%|                                                  | 0/1278 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n  0%|▏                                       | 6/1278 [00:30<1:43:23,  4.88s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"args = dict(\n\n  model_name_or_path=\"Qwen/Qwen2-VL-2B-Instruct\", # use official non-quantized Llama-3-8B-Instruct model\n\n  adapter_name_or_path=\"qwen2vl_lora\",            # load the saved LoRA adapters\n\n  template=\"qwen2_vl\",                     # same to the one in training\n\n  finetuning_type=\"lora\",                  # same to the one in training\n\n  export_dir=\"qwen2vl_7b_instruct_lora_merged_all_cat\",              # the path to save the merged model\n\n  export_size=2,                       # the file shard size (in GB) of the merged model\n\n  export_device=\"cpu\",                    # the device used in export, can be chosen from `cpu` and `cuda`\n\n  \n\n)\n","metadata":{"id":"zjxC19v-irhR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"json.dump(args, open(\"merge_qwen2vl.json\", \"w\", encoding=\"utf-8\"), indent=2)\n\n\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YZUxdUJPj4kh","outputId":"cb4723a4-f908-497b-80dd-f0d5493485a5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=\"\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd LLaMA-Factory","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!llamafactory-cli export merge_qwen2vl.json","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8l77DcskBWX","outputId":"6e45a99a-48e9-4372-889d-f036b04bed56","trusted":true},"outputs":[],"execution_count":null}]}