{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoProcessor, AutoModel\nfrom transformers import CLIPProcessor, CLIPModel\nfrom sklearn.preprocessing import LabelEncoder\nimport os\n\n# Paths\nimage_dir = '/kaggle/input/mesho-chll/MESHO/train_images/'  # Path to train images\ncsv_file = '/kaggle/input/mesho-chll/MESHO/train_MESH.csv'       # Path to CSV file\n\n# Parameters\nbatch_size = 64\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 1. Load the CSV that contains image IDs, Category, and target labels (attr_1 to attr_10)\ndf = pd.read_csv(csv_file)\n\n# Fill NaN values in attr_1 to attr_10 with 'no'\nattr_columns = [f'attr_{i}' for i in range(1, 11)]\ndf[attr_columns] = df[attr_columns].fillna('no')\n\n# Label encode the 'Category' column\ncategory_encoder = LabelEncoder()\ndf['Category'] = category_encoder.fit_transform(df['Category'])\n\n# Encode each attribute (attr_1 to attr_10) using LabelEncoder\nattr_encoders = {}\nfor col in attr_columns:\n    encoder = LabelEncoder()\n    df[col] = encoder.fit_transform(df[col])\n    attr_encoders[col] = encoder  # Store the encoder for later use\n\n# Create a new column for the full file path of each image\ndf['file_path'] = df['id'].apply(lambda x: f'{image_dir}{str(x).zfill(6)}.jpg')\n\n# 2. Load the CLIP model and processor\nclip_model = AutoModel.from_pretrained(\"google/siglip-base-patch16-512\")\nclip_processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-512\")\n\nclip_model.to(device)\nclip_model.eval()  # Set to evaluation mode since we only use the encoders\n\n# Custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, df, processor):\n        self.df = df\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = self.df.iloc[idx]['file_path']\n        image = Image.open(img_path).convert('RGB')\n\n        # Use CLIP processor to process the image (resizing, normalization)\n        inputs = self.processor(images=image, return_tensors=\"pt\")\n\n        # Get additional features (Category, len)\n        features = self.df.iloc[idx][['Category']].values.astype(np.float32)\n\n        # Get labels for attributes\n        labels = self.df.iloc[idx][attr_columns].values.astype(np.int64)\n\n        return (inputs, features), torch.tensor(labels)  # Return inputs for multi-label classification\n\n# 3. DataLoader for training and validation\ndef custom_dataloader(df, processor, batch_size):\n    dataset = CustomDataset(df, processor)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\ntrain_loader = custom_dataloader(df, clip_processor, batch_size)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4. Define the multi-output model using CLIP encodings\nimport torch.optim as optim\nfrom torch.cuda.amp import GradScaler, autocast\nclass MultiOutputModel(nn.Module):\n    def __init__(self, clip_model, num_features, num_outputs_per_attr):\n        super(MultiOutputModel, self).__init__()\n        \n        self.clip_model = clip_model\n        self.clip_model.eval()  # Set CLIP to eval mode\n\n        # Custom fully connected layers\n        clip_output_dim = 768 # Assuming using ViT-Large with 768-dimensional embeddings\n        self.fc_features = nn.Linear(clip_output_dim + num_features, 512)\n        self.fc1 = nn.Linear(512, 256)\n\n        # Output layers for each of the 10 attributes (multi-label output)\n        self.attr_outputs = nn.ModuleList([nn.Linear(256, num_outputs) for num_outputs in num_outputs_per_attr])\n\n    def forward(self, inputs, features):\n        image_inputs = inputs['pixel_values'].squeeze(1).to(device)\n\n        # Forward pass through CLIP image encoder\n        with torch.no_grad():\n            image_embeddings = self.clip_model.get_image_features(image_inputs)\n\n        # Concatenate image embeddings with the additional features\n        x = torch.cat([image_embeddings, features], dim=1)\n\n        # Pass through the fully connected layers\n        x = torch.relu(self.fc_features(x))\n        x = torch.relu(self.fc1(x))\n\n        # Pass through the attribute-specific output layers (multi-label output)\n        outputs = [attr_output(x) for attr_output in self.attr_outputs]\n\n        return outputs\n\n# 5. Instantiate the model\nnum_features = 1 # 'Category' and 'len'\nnum_classes_list = [len(attr_encoders[f'attr_{i}'].classes_) for i in range(1, 11)]\nmodel = MultiOutputModel(clip_model, num_features=num_features, num_outputs_per_attr=num_classes_list).to(device)\n\n# 6. Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef load_model_checkpoint(model, checkpoint_path):\n    \"\"\"Load the model from the specified checkpoint path.\"\"\"\n    if os.path.exists(checkpoint_path):\n        state_dict = torch.load(checkpoint_path)\n        \n        # Modify keys for DataParallel\n        new_state_dict = {}\n        for k, v in state_dict.items():\n            new_state_dict[f'module.{k}'] = v\n            \n        model.load_state_dict(new_state_dict)\n        print(f\"Model loaded from {checkpoint_path}\")\n    else:\n        print(f\"Checkpoint not found at {checkpoint_path}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom tqdm import tqdm\n\ndef load_model_checkpoint(model, checkpoint_path):\n    \"\"\"Load the model from the specified checkpoint path.\"\"\"\n    if os.path.exists(checkpoint_path):\n        model.load_state_dict(torch.load(checkpoint_path))\n        print(f\"Model loaded from {checkpoint_path}\")\n    else:\n        print(f\"Checkpoint not found at {checkpoint_path}\")\n\ndef train_model(model, train_loader, criterion, optimizer, epochs=15, save_dir='model_checkpoints'):\n    os.makedirs(save_dir, exist_ok=True)\n    model.train()\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{epochs}', unit='batch') as pbar:\n            for batch_idx, ((inputs, features), labels) in enumerate(train_loader):\n                inputs = {k: v.to(device) for k, v in inputs.items()}\n                features, labels = features.to(device), labels.to(device)\n                optimizer.zero_grad()\n\n                # Forward pass\n                outputs = model(inputs, features)\n\n                total_loss = 0.0\n                for idx, (output, label) in enumerate(zip(outputs, labels.T)):\n                    loss = criterion(output, label)\n                    total_loss += loss\n\n                total_loss.backward()\n                optimizer.step()\n\n                running_loss += total_loss.item()\n                pbar.set_postfix({'Batch Loss': total_loss.item()})\n                pbar.update(1)\n\n        avg_epoch_loss = running_loss / len(train_loader)\n        print(f'Epoch {epoch + 1} completed. Average Loss: {avg_epoch_loss:.4f}')\n\n        # Save the model after each epoch\n        checkpoint_path = os.path.join(save_dir, f'model_epoch_siglip_large_{epoch + 1}.pth')\n        torch.save(model.state_dict(), checkpoint_path)\n        print(f'Model saved at {checkpoint_path}')\n\n# Load model from checkpoint if specified\ncheckpoint_path = '/kaggle/input/newmods/model_checkpoints/model_epoch_siglip_base_12.pth'\nload_model_checkpoint(model, checkpoint_path)\n\n# Example usage\ntrain_model(model, train_loader, criterion, optimizer, epochs=15)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"inference","metadata":{}},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nfrom transformers import CLIPProcessor, CLIPModel\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm  # Import tqdm for progress bar\n\ndef load_trained_model(model_path, clip_model, num_features, num_classes_list):\n    # Load the state dictionary from the model path\n    state_dict = torch.load(model_path)\n    \n    # Create the model instance before loading the state dictionary\n    model = MultiOutputModel(clip_model, num_features=num_features, num_outputs_per_attr=num_classes_list)\n    \n    # Load the state dictionary with strict=False to avoid key mismatches\n    model.load_state_dict(state_dict)\n    \n    model.to(device)\n    model.eval()  # Set to evaluation mode\n    return model\n\n\n# Paths and parameters\nmodel_path = '/kaggle/input/siglip/model_epoch_siglip_base_3.pth'  # Path to your trained model\nimage_dir = '/kaggle/input/mesho-chll/MESHO/test_images/'  # Path to test images directory\ncsv_file = '/kaggle/input/mesho-chll/MESHO/test.csv'  # Path to test CSV file\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load the CLIP model and processor\n# Load the CLIP model and processor\nclip_model = AutoModel.from_pretrained(\"google/siglip-base-patch16-512\")\nclip_processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-512\")\nnum_features = 1  # 'Category'\nnum_classes_list = [len(attr_encoders[f'attr_{i}'].classes_) for i in range(1, 11)]\n\n# Load the model with the trained weights\nmodel = load_trained_model(model_path, clip_model, num_features, num_classes_list)\n\n# Load the test CSV that contains the 'id' and 'Category'\ntest_df = pd.read_csv(csv_file)\n\n# Preprocess a single image for inference\ndef preprocess_image(image_path, processor):\n    image = Image.open(image_path).convert('RGB')\n    inputs = processor(images=image, return_tensors=\"pt\")\n    return inputs\n\n# Inference function\ndef predict(model, image_path, processor, features):\n    # Preprocess the image\n    inputs = preprocess_image(image_path, processor)\n    inputs = inputs.to(device)\n\n    # Ensure features are properly shaped for model input\n    features = torch.tensor(features).unsqueeze(0).to(device)  # Shape as (1, num_features)\n\n    # Perform forward pass (inference)\n    with torch.no_grad():\n        attr_outputs = model(inputs, features)\n\n    # Convert outputs to predicted labels\n    predicted_labels = [torch.argmax(output, dim=1).item() for output in attr_outputs]\n   \n    return predicted_labels\n\n# Example: Perform inference on all test images\npredictions_list = []\n\n# Wrap the loop with tqdm for progress bar\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing Images\"):\n    image_id = str(row['id']).zfill(6) + '.jpg'  # Convert ID to format 000000.jpg\n    image_path = os.path.join(image_dir, image_id)\n\n    # Encode the Category (make sure it was encoded similarly as in training)\n    category_encoded_value = category_encoder.transform([row['Category']])[0]\n\n    # Perform prediction\n    predicted_attrs = predict(model, image_path, clip_processor, [category_encoded_value])\n\n    # Decode the predicted attributes back to their original labels\n    decoded_predictions = {f'attr_{i}': attr_encoders[f'attr_{i}'].inverse_transform([pred])[0] \n                           for i, pred in enumerate(predicted_attrs, 1)}\n    \n    # Store the results for this image, without Category for now\n    predictions_list.append({'id': row['id'], **decoded_predictions})\n\n# Convert predictions to a DataFrame for better output readability\npredictions_df = pd.DataFrame(predictions_list)\n\n# Merge predictions with the original test DataFrame based on 'id'\nmerged_df = pd.merge(test_df[['id', 'Category']], predictions_df, on='id')\n\n# Count attributes that are not predicted as 'no'\n# Assuming attribute columns are named attr_1, attr_2, ..., attr_10\nattribute_columns = [f'attr_{i}' for i in range(1, 11)]\n\n# Create the 'len' column based on the count of attributes that are not 'no'\nmerged_df['len'] = merged_df[attribute_columns].apply(lambda x: sum(attr != 'no' for attr in x), axis=1)\n\n# Reorder columns to have 'len' after 'Category' and before 'attr_1'\ncols = ['id', 'Category', 'len'] + attribute_columns\nmerged_df = merged_df[cols]\n\n# Display merged predictions for all test images\nprint(merged_df)\n\n# Save predictions to a CSV file\nmerged_df.to_csv('sub_siglip_ba.csv', index=False)\n","metadata":{},"execution_count":null,"outputs":[]}]}